{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56792328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5a3eb-f1b1-437d-9d11-cee3b50a2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "import mido\n",
    "from mido import MidiFile\n",
    "from scipy.interpolate import interp1d\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "from prepare_sax_phrase_dataset import *"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ae0148f",
   "metadata": {},
   "source": [
    "#----------------------------------------------------\n",
    "TODO move to someplace else\n",
    "Example: plot and play wav/spec of original and synthesized phrase\n",
    "#----------------------------------------------------\n",
    "%matplotlib notebook \n",
    "fig1, ax1 = plt.subplots(figsize = (5,3))\n",
    "fig2, ax2 = plt.subplots(figsize = (5,3))\n",
    "src_dir = '/home/mlspeech/itamark/ssynth/git_repos/DeepLearningExamples/PyTorch/SpeechSynthesis/HiFiGAN/data_ssynth'\n",
    "\n",
    "pnm = '02_Free_Improv_dynamic_mic_phrase696.pt'\n",
    "audio_fnm = pnm.replace('.pt', '.wav')\n",
    "\n",
    "yy, sr = librosa.load(f'{src_dir}/wavs/{audio_fnm}')\n",
    "ipd.display(ipd.Audio(yy, rate = sr))\n",
    "yy_synth, sr = librosa.load(f'{src_dir}/wavs_synth/{audio_fnm}')\n",
    "ipd.display(ipd.Audio(yy_synth, rate = sr))\n",
    "\n",
    "m1 =  torch.load(f'{src_dir}/mels/{pnm}')\n",
    "m2 =  torch.load(f'{src_dir}/mels_synth/{pnm}')\n",
    "\n",
    "ax1.imshow(m1, aspect='auto', origin='lower')\n",
    "ax1.set_title('original audio mel')\n",
    "ax2.imshow(m2, aspect='auto', origin='lower')\n",
    "ax2.set_title('synthesized (10 harmonics) audio mel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b94e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_notes = ['C3', 'A#5'] # alto sax range is ['Db3', 'A5'], take half-step below/above\n",
    "alto_sax_range = librosa.note_to_hz(range_notes)\n",
    "TEST_MODE = False #--- mode for experimenting on a small dataset\n",
    "tgt_sr = 44100\n",
    "\n",
    "if not TEST_MODE:\n",
    "    data_folder = '/home/mlspeech/itamark/ssynth/git_repos/DeepLearningExamples/PyTorch/SpeechSynthesis/HiFiGAN/data_ssynth/wavs_raw' #'/home/itamar/ssynth/data/wavs'\n",
    "    flist = glob.glob(f'{data_folder}/*Free*dynamic_mic.wav')\n",
    "else:\n",
    "    data_folder = '/home/mlspeech/itamark/ssynth/git_repos/DeepLearningExamples/PyTorch/SpeechSynthesis/HiFiGAN/data_ssynth_TMP/wavs_raw' #'/home/itamar/ssynth/data/wavs'\n",
    "    flist = glob.glob(f'{data_folder}/*dynamic_mic.wav')\n",
    "    \n",
    "out_dir = data_folder.replace('wavs_raw', 'wavs')\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "print_info = True\n",
    "dur = 0\n",
    "if print_info:\n",
    "    print(f'found {len(flist)} files in wavs_raw folder:')\n",
    "    print(pd.Series([os.path.basename(fl) for fl in flist]))\n",
    "    print('\\n')\n",
    "    for fnm in flist:\n",
    "        f = sf.SoundFile(fnm)\n",
    "        sec = f.frames / f.samplerate\n",
    "        dur += sec\n",
    "        #print(f'samples = {f.frames}')\n",
    "        print(f'file {os.path.basename(fnm)}')\n",
    "        print(f'\\tsample rate = {f.samplerate}, sample format = {f.subtype_info}, seconds = {sec:.1f}')\n",
    "print(f'total recording duration {dur / 60:.1f} minutes')\n",
    "print(f'NOTE: output files will be saved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d6118-feb4-4a5b-91c1-598a4d23328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_to_phrases(y, sr):\n",
    "    cfg = PhraseSegmentCfg(sr)\n",
    "    st, en, env_db = get_audio_pauses(y, cfg, verbose = False)\n",
    "    #assert(st[0] == cfg.win_hop_samples) #--- make sure first pause starts from audio's start \n",
    "    print(f'found {st.shape[0]} pauses')\n",
    "\n",
    "    #--- treat too long segments\n",
    "    st, en = split_long_phrases(y, st, en, cfg)\n",
    "    print(f'After splitting long phrases: {st.shape[0]} pauses')\n",
    "\n",
    "    #--- treat too short segments\n",
    "    st, en = merge_short_phrases(st, en, cfg)\n",
    "    print(f'After merging short phrases: {st.shape[0]} pauses')\n",
    "    \n",
    "    #--- update duration array\n",
    "    seg_dur_sec = (st[1:] - en[:-1]) / sr\n",
    "    phrase_inds = np.c_[en[:-1], st[1:]]\n",
    "    \n",
    "    return phrase_inds, seg_dur_sec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146029aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_disk = True if TEST_MODE else False\n",
    "process_if_exists = False #True\n",
    "\n",
    "#--- WARNING this will overrite existing raw audio files!\n",
    "write_resampled_files_to_disk = False # True \n",
    "\n",
    "phrase_df = []\n",
    "\n",
    "for file_nm in flist:\n",
    "    #--- check for previous results\n",
    "    file_nm_base = os.path.basename(file_nm)\n",
    "    exist_wavs = glob.glob(f'{out_dir}/{file_nm_base[:-4]}*.wav')\n",
    "    \n",
    "    if not process_if_exists and len(exist_wavs) > 0:\n",
    "        print(f'found {len(exist_wavs)} phrases from file {file_nm_base}. Skipping.')\n",
    "        continue\n",
    "  \n",
    "    print(f'>>> loading {file_nm_base}')\n",
    "    f = sf.SoundFile(file_nm)\n",
    "    resampling = False\n",
    "    if f.samplerate != tgt_sr:\n",
    "        resampling = True\n",
    "        print(f'file sr is {f.samplerate}, resampling to {tgt_sr}')\n",
    "    \n",
    "    y, sr = librosa.load(file_nm, sr = tgt_sr) #--- NOTE librosa converts 24 bit audio (which is int32) to float in [-1, 1]\n",
    "    if resampling and write_resampled_files_to_disk:\n",
    "        print('Warning: overwriting existing file with resampled one')\n",
    "        with sf.SoundFile(file_nm, 'w', tgt_sr, 1, f.subtype) as fout:\n",
    "            fout.write(y)\n",
    "    \n",
    "    phrase_inds, seg_dur_sec = split_audio_to_phrases(y, sr)\n",
    "    \n",
    "    print(f'Total phrases: {len(phrase_inds)}')\n",
    "    print(f'phrase durations sec: min {seg_dur_sec.min():.1f} max {seg_dur_sec.max():.1f} mean {seg_dur_sec.mean():.1f}')\n",
    "\n",
    "    #--- save phrases to disk\n",
    "    \n",
    "    if save_to_disk:\n",
    "        print(f'saving phrases to {out_dir}')\n",
    "    for k, pind in enumerate(phrase_inds):\n",
    "        yout = y[pind[0]:pind[1]]\n",
    "        ifnm = file_nm_base.replace('.wav', f'_phrase{k:03d}.wav')\n",
    "        fnm_out = f'{out_dir}/{ifnm}'\n",
    "        phrase_df.append(pd.Series(\n",
    "            dict(file_nm = file_nm_base, \n",
    "                 phrase_id = ifnm.replace('.wav', ''), \n",
    "                 sample_start = pind[0], \n",
    "                 sample_end = pind[1])))\n",
    "        if save_to_disk:\n",
    "            sf.write(fnm_out, yout, sr, subtype = f.subtype)\n",
    "            \n",
    "print('done')\n",
    "if len(phrase_df) > 0:\n",
    "    phrase_df = pd.concat(phrase_df, axis = 1).T\n",
    "    phrase_df.sort_values(by = 'phrase_id', inplace = True)\n",
    "all_files = glob.glob(f'{out_dir}/*.wav')\n",
    "print(f'data set size: {len(all_files)} phrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d53e2-51b3-4b7b-b781-e2de50c209ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_df_fnm = f'{data_folder}/../phrase_df.csv'\n",
    "\n",
    "if not os.path.isfile(phrase_df_fnm): # False:\n",
    "    phrase_df.to_csv(phrase_df_fnm)\n",
    "    print(f'saved phrase dataframe to {phrase_df_fnm}')\n",
    "else:\n",
    "    phrase_df = pd.read_csv(phrase_df_fnm)\n",
    "\n",
    "#display(phrase_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8980c95",
   "metadata": {},
   "source": [
    "## write metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- metadata.csv used by HiFiGan training script\n",
    "metadata_fnm = f'{data_folder}/../metadata.csv'\n",
    "if not os.path.isfile(metadata_fnm):\n",
    "    print(f'writing metadata.csv to {metadata_fnm}')\n",
    "    (phrase_df['phrase_id'] + '||').to_csv(metadata_fnm, index=False, header=False)\n",
    "else:\n",
    "    print(f'file {metadata_fnm} exists, not writing a new one')\n",
    "\n",
    "if False:\n",
    "    #--- write it below, when we have midi data as well\n",
    "    #--- filelist used by FastPitch (in HiFiGan there's a script that create file lists)\n",
    "    filelist_fnm = f'{data_folder}/../filelists_fastpitch/ssynth_audio.txt'\n",
    "    if not os.path.isfile(filelist_fnm):\n",
    "        ('wavs/' + phrase_df['phrase_id'] + '.wav|').to_csv(filelist_fnm, index=False, header=False)\n",
    "    else:\n",
    "        print(f'file {filelist_fnm} exists, not writing a new one')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa763d3e",
   "metadata": {},
   "source": [
    "## validation - compare detected phrses from phrase_df to actual files on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_nm in flist:\n",
    "    file_nm_base = os.path.basename(file_nm)\n",
    "    print(f'>>>> file {file_nm_base}')\n",
    "    y, sr = librosa.load(file_nm, sr = tgt_sr) \n",
    "    pdf = phrase_df.query(\"file_nm == @file_nm_base\")\n",
    "    print(f'total file duration {y.shape[0] / tgt_sr / 60:.1f} min')\n",
    "    print(f'total phrase duration {(pdf.sample_end - pdf.sample_start).sum()/tgt_sr/60:.1f} min')\n",
    "    print(f'phrases: {pdf.shape[0]}')\n",
    "\n",
    "    tlen = 0\n",
    "    for k in range(pdf.shape[0]):\n",
    "        p = pdf.iloc[k]\n",
    "        p_fnm = f'{out_dir}/{p.phrase_id}.wav'\n",
    "        y_p, _ = librosa.load(p_fnm, sr = tgt_sr) \n",
    "        tlen += len(y_p)\n",
    "        if p.sample_end - p.sample_start != len(y_p):\n",
    "            #pass\n",
    "            print(f'[{k}] mismatch between detected phrase and pharse-file on disk')\n",
    "\n",
    "        if False and k==24:\n",
    "            print(f'phrase from {os.path.basename(p_fnm)} ({len(y_p)} samples)')\n",
    "            ipd.display(ipd.Audio(y_p, rate=sr))\n",
    "            print(f'phrase from detection ({p.sample_end-p.sample_start} samples)')\n",
    "            ipd.display(ipd.Audio(y[p.sample_start:p.sample_end], rate=sr))\n",
    "            break\n",
    "    print(f'done, total len {tlen/tgt_sr/60:.1f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b83600",
   "metadata": {},
   "source": [
    "# Analysis of specific phrases\n",
    "## plot N seconds of signal, and detected phrases. Play audio of a selected phrase\n",
    "### file is selected in the cell above (if not selected - it will be the last file processed in the for-loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib.patches import Rectangle\n",
    "N = 70\n",
    "T1 = 10*60 # start of plot, sec\n",
    "T2 = T1 + N # end of plot, sec\n",
    "y0 = y[T1*sr:T2*sr]\n",
    "ts = T1 + np.arange(y0.shape[0]) / sr\n",
    "fig, ax = plt.subplots(figsize = (12,4))\n",
    "ax.plot(ts,y0)\n",
    "plist = pdf[(pdf.sample_start >= T1*sr) & (pdf.sample_end <= T2*sr)].reset_index()\n",
    "print(f'{len(plist)} phrases in segment')\n",
    "for ip, p in plist.iterrows():\n",
    "    ax.add_patch(Rectangle((p.sample_start/sr,-.2), (p.sample_end-p.sample_start)/sr, 0.4, edgecolor='blue', facecolor='red',alpha=.3 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6d271",
   "metadata": {},
   "source": [
    "### Select a phrse and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c26b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plist.iloc[3]\n",
    "p_fnm = f'{out_dir}/{p.phrase_id}.wav'\n",
    "y_p, _ = librosa.load(p_fnm, sr = tgt_sr)\n",
    "print(f'phrase start {p.sample_start/sr:.2f} sec')\n",
    "ipd.display(ipd.Audio(y_p, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d67d5",
   "metadata": {},
   "source": [
    "## Create MIDI phrases and write to metadata\n",
    "## NOTE: only needed if we synthesize from notes notation using Fastpitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_seg_inds(arr, shift_end_ind = True):\n",
    "    seg_inds = np.diff(np.r_[0, np.int_(arr), 0]).nonzero()[0]\n",
    "    n_segs = int(seg_inds.shape[0] / 2)\n",
    "    seg_inds = seg_inds.reshape((n_segs, 2)) # + np.c_[np.zeros(n_segs),-np.ones(n_segs)]   \n",
    "    if shift_end_ind:\n",
    "        seg_inds[:,1] -= 1\n",
    "    return seg_inds    \n",
    "\n",
    "def read_midi_to_df(midi_fnm, try_to_fix_note_order = True):\n",
    "    mid = MidiFile(midi_fnm)\n",
    "    \n",
    "    #assert(len(mid.tracks) == 1)\n",
    "    tr = mido.merge_tracks(mid.tracks);df =  pd.DataFrame([m.dict() for m in tr]);tempo = df.set_index('type').loc['set_tempo','tempo']\n",
    "    if type(tempo) == pd.Series:\n",
    "        uniq_tempo = tempo.unique()\n",
    "        if len(uniq_tempo) > 1:\n",
    "            raise Exception('multiple tempo changes not supported')\n",
    "        else:\n",
    "            tempo = uniq_tempo[0]\n",
    "            \n",
    "    df['ts_sec'] = mido.tick2second(df.time.cumsum(), mid.ticks_per_beat, tempo)\n",
    "    #--- some mete-messages like \"channel prefix\" contain non-zero time value. so remove them *after* calculating 'ts_sec'\n",
    "    for type_remove in ['channel_prefix', 'track_name', 'instrument_name', 'time_signature', 'key_signature', \n",
    "                        'smpte_offset', 'set_tempo', 'end_of_track', 'midi_port', 'program_change', 'control_change', 'pitchwheel', 'marker']:\n",
    "        df = df[df.type != type_remove]\n",
    "    \n",
    "    df = df.dropna(axis = 1).reset_index(drop = True)\n",
    "    \n",
    "    #--- sometimes, instead of a sequence of on-off notes, we get on-on-off-off. try to fix that\n",
    "    if try_to_fix_note_order:\n",
    "        try:\n",
    "            verify_midi(df)\n",
    "        except AssertionError:\n",
    "            print(f'{midi_fnm}: note order problem in midi dataframe, trying to fix...')\n",
    "            df_copy = df.copy()\n",
    "            #--- indices of where we expect to see \"note off\" and see \"note on\"\n",
    "            off_err_ind = df[((df.index % 2) == 1) & (df.type == 'note_on')].index\n",
    "            for ind in off_err_ind:\n",
    "                curr_note = df.loc[ind]\n",
    "                next_note = df.loc[ind + 1]\n",
    "                prev_note = df.loc[ind - 1]\n",
    "                if next_note.type == 'note_off' and next_note.note == prev_note.note:\n",
    "                    df.loc[ind + 1, 'ts_sec'] = curr_note.ts_sec - 0.001\n",
    "            df = df.sort_values(by = 'ts_sec', kind = 'stable').reset_index(drop = True)\n",
    "            try:\n",
    "                verify_midi(df)\n",
    "                print('fixed')\n",
    "            except AssertionError:\n",
    "                print('fix failed, calling verify_midi() on returned dataframe will fail')\n",
    "                #--- if fix failed, return the original copy\n",
    "                df = df_copy\n",
    "                \n",
    "    return df\n",
    "\n",
    "def verify_midi(midi_df):\n",
    "    #--- validate the assumption that we have series of note-on/note-off events\n",
    "    assert((midi_df['type'].iloc[::2] == 'note_on').all() and \n",
    "       (midi_df['type'].iloc[1::2] == 'note_off').all() and\n",
    "       (midi_df['note'].iloc[::2].to_numpy() == midi_df['note'].iloc[1::2].to_numpy()).all())\n",
    "\n",
    "def midi_phrase_from_dataframe(p, midi_df, sr):\n",
    "    t0 = p.sample_start / sr\n",
    "    t1 = p.sample_end / sr\n",
    "    midi_p = midi_df[(midi_df.ts_sec >= t0) & (midi_df.ts_sec <= t1)]\n",
    "    \n",
    "    #--- check for missing note_off (at end) or note_on (at start)\n",
    "    first_note = midi_p.iloc[0]\n",
    "    if first_note['type'] == 'note_off':\n",
    "        candidate = midi_df.loc[first_note.name - 1]\n",
    "        if candidate['type'] == 'note_on' and candidate['note'] == first_note['note']:\n",
    "            midi_p = pd.concat([candidate.to_frame().T, midi_p])\n",
    "            \n",
    "    last_note = midi_p.iloc[-1]\n",
    "    if last_note['type'] == 'note_on':\n",
    "        candidate = midi_df.loc[last_note.name + 1]\n",
    "        if candidate['type'] == 'note_off' and candidate['note'] == last_note['note']:\n",
    "            midi_p = pd.concat([midi_p, candidate.to_frame().T])\n",
    "    \n",
    "    return midi_p\n",
    "    \n",
    "def phrase_to_midi_string(p, midi_df, sr):    \n",
    "    midi_p = midi_phrase_from_dataframe(p, midi_df, sr)            \n",
    "    try:\n",
    "        verify_midi(midi_p)\n",
    "    except Exception as e:\n",
    "        print(f'phrase {p.phrase_id} verification failed')\n",
    "        return ''\n",
    "    \n",
    "    note_on = midi_p.loc[midi_p.type == 'note_on']\n",
    "    s = f\"wavs/{p.phrase_id}.wav|{' '.join(note_on.note.astype(int).astype(str).to_list())}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a48751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--- midi file reading to data-frame fails on the midi files in TEST_MODE (these are parallel recordings done by Elad, not the auto-midi files)\n",
    "midis_lns = []\n",
    "midi_folder = 'midi' if TEST_MODE else 'auto_midi'\n",
    "for fnm in flist:\n",
    "    fnm_base = os.path.basename(fnm)\n",
    "    midi_fnm = fnm.replace('/wavs_raw/', f'/{midi_folder}/').replace('.wav', '.mid')\n",
    "    if TEST_MODE and '_dynamic_mic' in midi_fnm:\n",
    "        midi_fnm = midi_fnm.replace('_dynamic_mic', '')\n",
    "\n",
    "    print(f'reading midi file {os.path.basename(midi_fnm)}')\n",
    "    midi_df = read_midi_to_df(midi_fnm)\n",
    "    verify_midi(midi_df)\n",
    "    p_df = phrase_df.query(\"file_nm == @fnm_base\").reset_index(drop = True)\n",
    "    print(f'processing {p_df.shape[0]} phrases')\n",
    "    for ip, p in p_df.iterrows():\n",
    "        midis = phrase_to_midi_string(p, midi_df, tgt_sr)\n",
    "        midis_lns.append(midis)\n",
    "\n",
    "#--- write metadata to file\n",
    "filelist_fnm = f'{data_folder}/../filelists_fastpitch/ssynth_audio.txt'\n",
    "print(f'writing {len(midis_lns)} lines to file')\n",
    "if not os.path.isfile(filelist_fnm):\n",
    "    with open(filelist_fnm, 'w') as fout:\n",
    "        fout.writelines([ln + '\\n' for ln in midis_lns])\n",
    "else:\n",
    "    print(f'file {filelist_fnm} exists, not writing a new one')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be8847",
   "metadata": {},
   "source": [
    "## Example: choose a phrase, and:\n",
    "* ### find its midi counterpart\n",
    "* ### play the phrase\n",
    "* ### plot the phrase and the midi note-on/note-off marks\n",
    "* ### plot a spectrogram, and midi notes and detected pitch on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c14fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = phrase_df.query(\"file_nm == @fnm_base\")\n",
    "p = phrase_df.iloc[2] # pdf.iloc[250]\n",
    "t0 = p.sample_start / sr\n",
    "wav_fnm = f'{out_dir}/{p.phrase_id}.wav'\n",
    "seg, sr = librosa.load(wav_fnm, sr = tgt_sr)\n",
    "\n",
    "try:\n",
    "    midi_p = midi_phrase_from_dataframe(p, midi_df, sr)   \n",
    "    #print(midi_p)\n",
    "    verify_midi(midi_p)\n",
    "except Exception as e:\n",
    "    midi_p = None\n",
    "    print(f'Warning: failed to load midi pharse: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fnm0 = file_nm #flist[-1]\n",
    "#midi_fnm = fnm0.replace('/wavs_raw/', '/auto_midi/').replace('.wav', '.mid')\n",
    "#midi_df = read_midi_to_df(midi_fnm)\n",
    "#p =  pdf.iloc[99]\n",
    "#t0 = p.sample_start / tgt_sr\n",
    "#t1 = p.sample_end / tgt_sr\n",
    "\n",
    "#print(t0.round(2),t1.round(2),p) #midi_p\n",
    "\n",
    "#p = ph_df.iloc[0] #phrase_df.query('phrase_id == \"05_Free_Improv_dynamic_mic_phrase181\"').iloc[0]\n",
    "if midi_p is not None:\n",
    "    note_on = midi_p.loc[midi_p.type == 'note_on']\n",
    "    note_off = midi_p.loc[midi_p.type == 'note_off']\n",
    "\n",
    "    note_hz = librosa.midi_to_hz(note_on.note)\n",
    "    note_on_ts = note_on['ts_sec'].values - t0\n",
    "    note_off_ts = note_off['ts_sec'].values - t0\n",
    "\n",
    "    phrase_to_midi_string(p, midi_df, tgt_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- pitch detection\n",
    "win = 1024\n",
    "ac_win = 512 # autocorrelation window\n",
    "hop = 256\n",
    "#--- Note librosa.pyin is the method used by the hifigan example for speech synthesis\n",
    "f0, vflag, vprob = librosa.pyin(seg, \n",
    "                                fmin = alto_sax_range[0], \n",
    "                                fmax = alto_sax_range[1], \n",
    "                                sr = sr, \n",
    "                                frame_length=win, \n",
    "                                win_length=ac_win, \n",
    "                                hop_length=hop, \n",
    "                                center=False,\n",
    "                                max_transition_rate=100)\n",
    "times = librosa.times_like(f0, sr = sr, hop_length=hop)\n",
    "times += (win / 2) / sr # center at windows mid point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbd4f4-f9a0-4a34-a705-b06716f2be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,2))\n",
    "print(wav_fnm)\n",
    "librosa.display.waveshow(seg, sr=sr)\n",
    "plot_midi_notes_segs = False\n",
    "if plot_midi_notes_segs and midi_p is not None:\n",
    "    for k in range(len(note_on)):\n",
    "        xk = note_on_ts[k]\n",
    "        yk = -0.2\n",
    "        ax.add_patch(Rectangle((xk, yk), note_off_ts[k] - note_on_ts[k], 0.4, edgecolor='blue', facecolor='red',alpha=.3))\n",
    "        ax.text(xk,yk,note_on.iloc[k].note)\n",
    "        \n",
    "sr_play = sr / 1\n",
    "ipd.display(ipd.Audio(seg, rate=sr_play))\n",
    "if midi_p is not None:\n",
    "    n = note_on_ts.shape[0]\n",
    "    ax.plot(note_on_ts, [0.01]*n, 'ro', label = 'auto-midi on')\n",
    "    ax.plot(note_off_ts, [-0.01]*n, 'cx', label = 'auto-midi off')\n",
    "\n",
    "ax.plot(times, 0.2*vprob, 'k.')\n",
    "no_note = (~vflag) # | (vprob < 0.15)\n",
    "ax.plot(times[no_note], 0.2*vprob[no_note], 'rx')\n",
    "#ax.plot(note_on_ts, seg[(note_on_ts*sr).astype(int)], 'ro', label = 'auto-midi on')\n",
    "#ax.plot(note_off_ts, seg[(note_off_ts*sr).astype(int)], 'cx', label = 'auto-midi off')\n",
    "#ax.set_title(f'seg {ind} at {(phrase_inds[ind] / sr).round(1)} sec')\n",
    "\n",
    "wav_of_all_notes = False\n",
    "if wav_of_all_notes and midi_p is not None:\n",
    "    for k in range(len(note_on)):\n",
    "        st = int(note_on_ts[k] * sr)\n",
    "        en = int(note_off_ts[k] * sr)\n",
    "        print(note_on_ts[k])\n",
    "        ipd.display(ipd.Audio(seg[st:en], rate=sr_play))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90f8e06e",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "n1 = 340\n",
    "n2 = 349\n",
    "ax.plot(np.arange(st[n1], en[n2-1]) / sr, y[st[n1]:en[n2-1]])\n",
    "for k in range(n1,n2):\n",
    "    ax.plot(np.arange(st[k],en[k]) / sr, y[st[k]:en[k]],'r:')\n",
    "\n",
    "ax.plot(np.arange(en[n2-1],len(y)) / sr, y[en[n2-1]:],'g')\n",
    "#ax.set_xlim([1366,1370])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(seg)), ref=np.max)\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "yscale = 'log' # 'linear'\n",
    "img = librosa.display.specshow(D, x_axis='time', y_axis = yscale, ax=ax, sr=sr)\n",
    "ax.set(title='pYIN fundamental frequency estimation')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "th = 0.25\n",
    "#ax.plot(times[vprob >= th], f0[vprob >= th], 'o',label='voiced', color='red')\n",
    "ax.plot(times, f0, '.',label='f0', color='cyan', linewidth=3)\n",
    "if midi_p is not None:\n",
    "    ax.plot(note_on_ts, note_hz, 'ro', label = 'auto-midi on')\n",
    "    ax.plot(note_off_ts, note_hz, 'gx', label = 'auto-midi off')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "#ax.set_ylim([120,1700])\n",
    "#ax.set_xlim([4.5,5.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3a6f6",
   "metadata": {},
   "source": [
    "## play some random phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## play some random phrases\n",
    "k = 100\n",
    "for ind in range(k, k + 201, 50):\n",
    "    print(all_files[ind].split('/')[-1])\n",
    "    ipd.display(ipd.Audio(all_files[ind], rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafe86b",
   "metadata": {},
   "source": [
    "## toy model of fastpitch + hifigan from \"raw midi notes\" (sounds bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f479282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- load and play model output of complete loop (fastpitch + hifigan)\n",
    "#--- this sounds horrible, as expected. fastpitch was trained on 'raw modi notes' (no timing, pitch, or energy info)\n",
    "out_files = glob.glob('/home/mlspeech/itamark/ssynth/git_repos/DeepLearningExamples/PyTorch/SpeechSynthesis/FastPitch/results/2022-12-20_fastpitch_ssynth/audio_devset10_fp32_fastpitch_hifigan_SAX_denoise-0.01/*.wav')\n",
    "for fl in out_files:\n",
    "    print(os.path.basename(fl))\n",
    "    ipd.display(ipd.Audio(fl, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e7d2f",
   "metadata": {},
   "source": [
    "# Synthesize a sine wave with pitch and env from signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_harmonics(min_freq_src_hz, max_freq_src_hz, sr, max_freq_tgt_hz):\n",
    "    fmin = min_freq_src_hz # librosa.note_to_hz(range_notes[0]) # can't naively use fnew.min() since we interpolate to f=0 Hz\n",
    "    num_harmonics = int(max_freq_tgt_hz / fmin)\n",
    "    new_sr = 2 * max_freq_src_hz * num_harmonics\n",
    "    #--- take the smallest multiple of sr which is high enough (6 is the highest, assuming freqs.max() <= 932 Hz)\n",
    "    new_sr_factor = [k for k in range(1, 7) if k * sr > new_sr][0]\n",
    "    return num_harmonics, new_sr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import decimate, butter, dlti # resample_poly\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "#num_harmonics = 10; #None #\n",
    "#verbose = False; max_freq_hz=None #8000\n",
    "\n",
    "#if True:\n",
    "def phrase_to_synth(seg, sr, midi_p, t0, num_harmonics = None, max_freq_hz = None, spline_smoothing = None, verbose = False):\n",
    "    ''' Exactly one of these should be given (and the other set to None):\n",
    "            - num harmonics: how many harmonics (inc the fundamental) are used in the saw-tooth additive synthesis\n",
    "                             in this case the max-freq is note-dependent (f0*num_harmonics) and the caller is responsible\n",
    "                             to make sure that (highest note in hz) * (num_harmonics) < nyquist\n",
    "            - max_freq_hz:   synthesize up to this frequency. This is done by upsampling, synthesizing the required amound of harmonics,\n",
    "                             and downsampling back to sr\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(f'pitch detection range: {alto_sax_range.round(1)} Hz, {(sr/alto_sax_range).astype(int)} samples')\n",
    "        print(f'pitch detection: frame len {win}, auto-corr len {ac_win} (min freq of {sr/ac_win:.1f} Hz), hop len {hop}')\n",
    "    \n",
    "    assert(num_harmonics is None or max_freq_hz is None)\n",
    "    f1, vflag1, vprob1 = librosa.pyin(seg, \n",
    "                                      fmin = alto_sax_range[0], \n",
    "                                      fmax = alto_sax_range[1], \n",
    "                                      sr = sr, \n",
    "                                      frame_length=win, \n",
    "                                      win_length=ac_win, \n",
    "                                      hop_length=hop, \n",
    "                                      center=True, \n",
    "                                      max_transition_rate=100)\n",
    "    times1 = librosa.times_like(f1, sr = sr, hop_length = hop)\n",
    "    no_note1 = (~vflag1)\n",
    "    tmin = times1[0]\n",
    "    tmax = times1[-1]\n",
    "    \n",
    "    note_on = midi_p.loc[midi_p.type == 'note_on']\n",
    "    note_off = midi_p.loc[midi_p.type == 'note_off']\n",
    "    #note_hz = librosa.midi_to_hz(note_on.note)\n",
    "    note_on_ts = note_on['ts_sec'].values - t0\n",
    "    note_off_ts = note_off['ts_sec'].values - t0\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "    #--- interpolate missing pitch, where possible. otherwise, set to 0 (in order to accumulate 0 phase when integrating)\n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "    #--- step A, interpolate within (intra-) midi notes\n",
    "    n_notes = note_on.shape[0]\n",
    "    if verbose:\n",
    "        print(f'samples with non-detected pitch: {np.isnan(f1).sum()}')\n",
    "    for k in range(n_notes):\n",
    "        #--- first, find missing pitch samples which are inside a detected midi note\n",
    "        midi_note_span = (times1 >= note_on_ts[k]) & (times1 <= note_off_ts[k])\n",
    "        \n",
    "        #--- if no missing pitch samples are in the midi note span, we don't need this note, so skip\n",
    "        if not (midi_note_span & no_note1).any():\n",
    "            continue\n",
    "        \n",
    "        #--- if we don't have at least 2 pitch samples in the note span, we can't extrapolate, so skip\n",
    "        if (midi_note_span & ~no_note1).sum() < 2:\n",
    "            continue\n",
    "            \n",
    "        #--- build the interpolating function from detected pitch samples\n",
    "        pitch_intrp = interp1d(times1[midi_note_span & ~no_note1], \n",
    "                               f1[midi_note_span & ~no_note1], \n",
    "                               fill_value = 'extrapolate', \n",
    "                               kind = 'nearest',\n",
    "                               assume_sorted = True)\n",
    "        #--- the time samples where we want to interpolate: inside midi note AND missing pitch\n",
    "        t_intrp = times1[midi_note_span & no_note1]\n",
    "        f1[midi_note_span & no_note1] = pitch_intrp(t_intrp)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'after interpolating using midi notes: samples with non-detected pitch: {np.isnan(f1).sum()}')\n",
    "\n",
    "    #--- step B, interpolate across (inter-) midi notes\n",
    "    max_gap_to_interpolate_sec = 0.1 #--- don't interpolate gaps above this interval in seconds\n",
    "    no_note1 = np.isnan(f1)\n",
    "    seg_inds = binary_array_to_seg_inds(no_note1, shift_end_ind = False)\n",
    "    seg_lens_sec = np.diff(seg_inds, 1)[:,0] * hop / sr\n",
    "    for k, inds in enumerate(seg_inds):\n",
    "        #--- don't interpolate head or tail of signal, or if gap is too long\n",
    "        #--- TODO check energy envelope in gap (interpolate only above env threshold)\n",
    "        gap_len = seg_lens_sec[k]\n",
    "        if (inds[0] == 0) or (inds[1] == len(f1)) or gap_len > max_gap_to_interpolate_sec:\n",
    "            continue\n",
    "        gap_len_samples = inds[1] - inds[0]\n",
    "        if verbose:\n",
    "            print(f'interpolating over {gap_len_samples} samples over gap of {gap_len:.3f} sec')\n",
    "        #--- linear interpolation using 1 sample before and after\n",
    "        new_freqs = np.linspace(f1[inds[0] - 1], f1[inds[1]], gap_len_samples + 2)\n",
    "        f1[inds[0]:inds[1]] = new_freqs[1:-1]\n",
    "\n",
    "    no_note1 = np.isnan(f1)\n",
    "    seg_inds = binary_array_to_seg_inds(no_note1, shift_end_ind = False)\n",
    "    if verbose:\n",
    "        print(f'after interpolating over small gaps: samples with non-detected pitch: {np.isnan(f1).sum()}')\n",
    "    #--- lastly, fill with zeros the samples that are still missing\n",
    "    f1[np.isnan(f1)] = 0.\n",
    "    \n",
    "    #--- set number of harmonics of sawtooth wave\n",
    "    if num_harmonics is not None:\n",
    "        additive_synth_k = num_harmonics # 10\n",
    "        should_downsample = False\n",
    "    else:\n",
    "        num_harmonics, new_sr_factor = get_num_harmonics(f1[f1 > 20].min(), f1.max(), sr, max_freq_hz)\n",
    "        #--- make sure we stay below new nyquist\n",
    "        assert f1.max() * num_harmonics < 0.5 * sr * new_sr_factor, f'Nyquist says you cannot synthesize {num_harmonics} harmonics at {new_sr_factor} X (current sampling rate)'\n",
    "        additive_synth_k = num_harmonics\n",
    "        sr *= new_sr_factor\n",
    "        should_downsample = True\n",
    "    \n",
    "    #--- now interpolate to sampling-rate grid\n",
    "    dt = 1 / sr\n",
    "    fintrp = interp1d(times1, f1)\n",
    "    tnew = np.arange(tmin, tmax, dt)\n",
    "    fnew = fintrp(tnew)\n",
    "    \n",
    "    #--- phase is the integral of instantanous freq\n",
    "    phi = np.cumsum(2 * np.pi * fnew * dt)\n",
    "    # to wrap: phi = (phi + np.pi) % (2 * np.pi) - np.pi \n",
    "        \n",
    "    x = np.sin(phi) #(np.sin(phi) + .5*np.sin(2*phi) + .333*np.sin(3*phi) + .25*np.sin(4*phi))\n",
    "    for k in range(2, additive_synth_k + 1):\n",
    "        x += (-1)**(k-1) * np.sin(k*phi) / k\n",
    "    \n",
    "    #--- if we upsampled, go back to original rate\n",
    "    if should_downsample:\n",
    "        #--- for x, give a \"anti-alias\" filter to \"decimate\", but actually use it to filter above the desired max_freq_hz\n",
    "        zpk = butter(12, max_freq_hz, output = 'zpk', fs = sr)\n",
    "        aa_filt = dlti(*zpk) \n",
    "        x = decimate(x, new_sr_factor, ftype = aa_filt)\n",
    "        fnew = decimate(fnew, new_sr_factor) #--- fnew is just used to zero the envelope, so decimate so size fits\n",
    "        sr = int(sr / new_sr_factor)\n",
    "    \n",
    "    env = librosa.feature.rms(y = seg, frame_length = 512, hop_length = 1, center = True)\n",
    "    env = 1.3 * np.sqrt(2)*env[0, :len(x)]\n",
    "    env[fnew == 0] = 0. # don't apply envelope where there was no pitch found\n",
    "\n",
    "    #--- make envelope go to zero smoothly. This also takes care of the non-continous phase at jumps of f1 to 0\n",
    "    env_segments = binary_array_to_seg_inds(env == 0)\n",
    "    decay_time_sec = 0.05 #--- 50 msec decay time\n",
    "    decay_time_samples = int(decay_time_sec * sr)\n",
    "    for env_seg in env_segments:\n",
    "        if env_seg[0] == 0:\n",
    "            continue\n",
    "        ind_start = max(0, env_seg[0] - decay_time_samples)\n",
    "        decay_len = env_seg[0] - ind_start\n",
    "        decay_factor = np.linspace(1, 0, decay_len)\n",
    "        env[ind_start: env_seg[0]] *= decay_factor    \n",
    "    \n",
    "    if spline_smoothing is not None:\n",
    "        ts = t0 + np.arange(0, len(env)) / sr\n",
    "        spl = UnivariateSpline(ts, env, s = spline_smoothing, k = 2)\n",
    "        env = spl(ts)\n",
    "         \n",
    "    x *= env\n",
    "    gain = np.sqrt((x**2).mean()) / np.sqrt((seg**2).mean()) \n",
    "    x /= gain\n",
    "    env /= gain\n",
    "    \n",
    "    return x, env, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a559227",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, env, _ = phrase_to_synth(seg, sr, midi_p, t0, num_harmonics = 30, verbose=True)\n",
    "xnew = np.c_[x, x].T #.3*seg[:len(x)]].T\n",
    "ipd.display(ipd.Audio(xnew, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7857647",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, env, _ = phrase_to_synth(seg, sr, midi_p, t0, max_freq_hz = 16000, spline_smoothing = 0.5, verbose=True)\n",
    "xnew = np.c_[x1, x1].T #.3*seg[:len(x)]].T\n",
    "ipd.display(ipd.Audio(xnew, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32920921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "fig, ax = plt.subplots(figsize = (12,2))\n",
    "n = min(len(seg), len(x))\n",
    "tsec = np.arange(n) / sr\n",
    "plt.plot(tsec, seg[:n],'b.')\n",
    "plt.plot(tsec, x[:n],'g.')\n",
    "plt.plot(tsec,env,'r:')\n",
    "#plt.xlim([0,0.05])\n",
    "plt.show()\n",
    "\n",
    "if True:\n",
    "    for xx in [x,x1]:\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(xx, n_fft=2048, win_length=1024, hop_length=256)))\n",
    "        fig, ax = plt.subplots(figsize = (8,4))\n",
    "        yscale =  'linear' #'log' #\n",
    "        img = librosa.display.specshow(D, x_axis='time', y_axis = yscale, ax=ax, sr=sr)\n",
    "        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbab23",
   "metadata": {},
   "source": [
    "# Create synthetic (saw tooth) wavs for all phrases and save to disk (inc. pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- exactly one of these should be None:\n",
    "num_harmonics = None # 10\n",
    "max_freq_hz = 16000\n",
    "spline_smoothing = 0.5 # set to None for no smoothing of amplitude envelope\n",
    "\n",
    "suffix = f'{num_harmonics}h' if max_freq_hz is None else f'{int(max_freq_hz / 1000)}k'\n",
    "suffix += '' if spline_smoothing is None else f'_spl{spline_smoothing}'\n",
    "\n",
    "synth_out_dir = data_folder.replace('wavs_raw', f'wavs_synth_{suffix}')\n",
    "pitch_out_dir = data_folder.replace('wavs_raw', 'pitch_synth')\n",
    "print(f'writing synthesized wavs to {synth_out_dir}')\n",
    "print(f'writing extracted pitch to {pitch_out_dir}')\n",
    "\n",
    "if not os.path.isdir(synth_out_dir):\n",
    "    os.mkdir(synth_out_dir)\n",
    "if not os.path.isdir(pitch_out_dir):\n",
    "    os.mkdir(pitch_out_dir)\n",
    "\n",
    "#--- iterate over files, and over phrases in an inner loop \n",
    "for ifnm, fnm in enumerate(flist):\n",
    "    #if ifnm < 2:\n",
    "    #    continue\n",
    "    fnm_base = os.path.basename(fnm)\n",
    "    midi_fnm = fnm.replace('/wavs_raw/', f'/{midi_folder}/').replace('.wav', '.mid')\n",
    "    if TEST_MODE and '_dynamic_mic' in midi_fnm:\n",
    "        midi_fnm = midi_fnm.replace('_dynamic_mic', '')\n",
    "        \n",
    "    print(f'[{ifnm}] reading midi file {os.path.basename(midi_fnm)}')\n",
    "    midi_df = read_midi_to_df(midi_fnm)\n",
    "    verify_midi(midi_df)\n",
    "    p_df = phrase_df.query(\"file_nm == @fnm_base\").reset_index(drop = True)\n",
    "    print(f'processing {p_df.shape[0]} phrases')\n",
    "    for iphrs, phrs in p_df.iterrows():             \n",
    "        #if iphrs < 610:\n",
    "        #    continue\n",
    "        wav_fnm = f'{out_dir}/{phrs.phrase_id}.wav'\n",
    "        seg, sr = librosa.load(wav_fnm, sr = tgt_sr)\n",
    "        midi_p = midi_phrase_from_dataframe(phrs, midi_df, sr)\n",
    "        t0 = phrs.sample_start / sr\n",
    "        try:\n",
    "            seg_synth, env, pitch = phrase_to_synth(seg, sr, midi_p, t0, \n",
    "                                                    num_harmonics = num_harmonics, \n",
    "                                                    max_freq_hz = max_freq_hz,\n",
    "                                                    spline_smoothing = spline_smoothing,  \n",
    "                                                    verbose = False)\n",
    "        except Exception as e:\n",
    "            print(f'phrase {iphrs} failed with error: {e}')\n",
    "            continue\n",
    "        #--- save synth signal and pitch\n",
    "        fnm_out = f'{synth_out_dir}/{phrs.phrase_id}.wav'\n",
    "        sf.write(fnm_out, seg_synth, sr, subtype = 'PCM_24')\n",
    "        pitch_fnm_out = f'{pitch_out_dir}/{phrs.phrase_id}.pt'\n",
    "        pitch = torch.tensor(pitch[np.newaxis,:].astype(np.float32))\n",
    "        torch.save(pitch, pitch_fnm_out)\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = min(len(seg), len(seg_synth))\n",
    "xnew = np.c_[.5*seg_synth[:n], .4*seg[:n]].T\n",
    "ipd.display(ipd.Audio(xnew, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba6729",
   "metadata": {},
   "source": [
    "## Analyze the difference between pitch extracted from whole phrase vs. a segment\n",
    "The algorithm uses probability of pitch-jumps so results may vary, depending on the segment start/end"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4406716e",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#np.isnan(f1).nonzero()[0]\n",
    "print(no_note.nonzero()[0])\n",
    "nn1, nn2 = 182-3, 187+5 #184,189\n",
    "#nn1, nn2 = 182, 215 #184,189\n",
    "samples_seg = (times1[[nn1,nn2]]*sr).round().astype(int)\n",
    "print('\\n',f1[nn1:nn2], samples_seg)\n",
    "\n",
    "tt1 = samples_seg[0] - 0*hop\n",
    "tt2 = samples_seg[1] + 0*hop\n",
    "seg_c = seg[tt1:tt2]\n",
    "#f_c1, vflag_c1, vprob_c1 = librosa.pyin(seg_c, fmin = 420, fmax = 530, sr = sr, frame_length=256, win_length=128, hop_length=64, center=True, max_transition_rate=100)\n",
    "f_c1, vflag_c1, vprob_c1 = librosa.pyin(seg_c, fmin = alto_sax_range[0], fmax = alto_sax_range[1], sr = sr, frame_length=win, win_length=ac_win, hop_length=hop, center=True,max_transition_rate=100)\n",
    "print('\\n',f_c1, (tt1,tt2))\n",
    "\n",
    "#f_c2, vflag_c2, vprob_c2 = librosa.pyin(seg_c, fmin = 270, fmax = 340, sr = sr, frame_length=512, win_length=256, hop_length=128, center=True)\n",
    "#print('\\n',f_c2)\n",
    "\n",
    "%matplotlib notebook \n",
    "plt.plot(seg_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d88d98c",
   "metadata": {},
   "source": [
    "%matplotlib notebook\n",
    "ff1, vf1, vp1 = librosa.pyin(seg, fmin = alto_sax_range[0], fmax = alto_sax_range[1], sr = sr, frame_length=win, win_length=ac_win, hop_length=hop, center=True,max_transition_rate=100)\n",
    "k0=182\n",
    "k1=215\n",
    "ff2, vf2, vp2 = librosa.pyin(seg[k0*hop:k1*hop], fmin = alto_sax_range[0], fmax = alto_sax_range[1], sr = sr, frame_length=win, win_length=ac_win, hop_length=hop, center=True,max_transition_rate=100)\n",
    "plt.plot(ff1,'bo')\n",
    "plt.plot(np.arange(k0,k1+1),ff2,'r.')\n",
    "plt.grid()\n",
    "k0*hop,k1*hop,seg_c[:5],seg[k0*hop:][:5],ff2[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b9ece1d",
   "metadata": {},
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "#D = librosa.amplitude_to_db(np.abs(librosa.stft(seg_c, n_fft=2048, win_length=512, hop_length=128)), ref=np.max)\n",
    "D = np.abs(librosa.stft(seg_c, n_fft=2048, win_length=512, hop_length=128))\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "yscale =  'linear' #'log' #\n",
    "img = librosa.display.specshow(D, x_axis='time', y_axis = yscale, ax=ax, sr=sr)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "ipd.display(ipd.Audio(seg_c, rate=sr/4)),alto_sax_range"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7c50fb0",
   "metadata": {},
   "source": [
    "#-- try to use low-pass to deal with mis-deteced pitch (results: it doens't help...)\n",
    "f1, vflag1, vprob1 = librosa.pyin(seg, fmin = alto_sax_range[0], fmax = alto_sax_range[1], \n",
    "                                  sr = sr, frame_length=1024, win_length=256, hop_length=64, center=True, max_transition_rate=100,n_thresholds=50)\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "b,a = butter(N=8, Wn=950, fs=sr)\n",
    "seg_lp = filtfilt(b,a,seg)\n",
    "f1lp, vflag1lp, vprob1lp = librosa.pyin(seg, fmin = alto_sax_range[0], fmax = alto_sax_range[1], \n",
    "                                  sr = sr, frame_length=512, win_length=256, hop_length=64, center=True, max_transition_rate=100, n_thresholds=50)\n",
    "\n",
    "#f2, vflag2, vprob2 = librosa.pyin(seg, fmin = alto_sax_range[0], fmax = alto_sax_range[1], \n",
    "#                                  sr = sr, frame_length=int(win/2), win_length=int(ac_win/2), hop_length=64, center=True, max_transition_rate=100)\n",
    "\n",
    "#f3, vflag3, vprob3 = librosa.pyin(seg, fmin = alto_sax_range[0], fmax = alto_sax_range[1], \n",
    "#                                  sr = sr, frame_length=256, win_length=128, hop_length=64, center=False, n_thresholds=10,)\n",
    "f1[np.isnan(f1)] = 0.\n",
    "f1lp[np.isnan(f1lp)] = 0.\n",
    "#f2[np.isnan(f2)] = 0.\n",
    "#f3[np.isnan(f3)] = 0.\n",
    "%matplotlib notebook \n",
    "plt.plot(f1,'bo')\n",
    "plt.plot(f1lp,'r.')\n",
    "#plt.plot(f2,'rx')\n",
    "#plt.plot(f3,'g.')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d41939dd",
   "metadata": {},
   "source": [
    "%matplotlib notebook \n",
    "s0=seg[116500-1024:116500+1024]\n",
    "s0_lp=seg_lp[116500-1024:116500+1024]\n",
    "plt.plot(s0)\n",
    "plt.plot(s0_lp)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_synth.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ca288",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(ipd.Audio('ewimididemo.wav', rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fbf27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
