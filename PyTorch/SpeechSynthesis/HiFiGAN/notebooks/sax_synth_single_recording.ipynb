{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from mido import MidiFile\n",
    "import mido\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "#--- import HiFiGAN modules\n",
    "sys.path.append('../')\n",
    "import models\n",
    "import common.layers as layers \n",
    "from common.utils import load_wav #--- use same method that is used in hifigan for loading audio\n",
    "from hifigan.data_function import mel_spectrogram\n",
    "from hifigan.models import Denoiser\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "ipd.display(ipd.HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea30139",
   "metadata": {},
   "source": [
    "# load cfg and generator model from checkpoint\n",
    "also create denoiser instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63457324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- get config from checkpoint, so no need to load args from disk\n",
    "#args = pickle.load(open('../TMP_args.p', 'rb'))\n",
    "#gen_config = models.get_model_config('HiFi-GAN', args)\n",
    "\n",
    "DEVICE = 'cuda' # 'cpu' or 'cuda'\n",
    "\n",
    "#--- in hifigan code they use 2 implementations\n",
    "#--- (1) from fastpitch, when pre-calculating mel spec in prepare_dataset.sh. This is saved to disk and used for training/inference as mel spec INPUT \n",
    "#--- (2) from hifigan, during training, when calculating mel spec for OUTPUT (target) signal, and for inference\n",
    "#--- NOTE use the same implementation that was used for INPUT in training. >>> starting 2023-05-28 and after, this should be 'hifigan' <<<\n",
    "MEL_IMPL = 'hifigan' #'fastpitch' # \n",
    "\n",
    "assert DEVICE == 'cuda', 'ERROR: cpu not supported yet (mel code assumes torch tensors)'\n",
    "\n",
    "#m_path = '../results/2023_01_20_hifigan_ssynth44khz_synthesized_input/hifigan_gen_checkpoint_10000.pt'\n",
    "#m_path = '../results/2023_05_15_hifigan_ssynth44khz_synthesized_input_16k_spl0.5/hifigan_gen_checkpoint_3000.pt'\n",
    "m_path = '../results/2023_05_28_hifigan_ssynth44khz_synthesized_input_16k_spl0.5_nonorm/hifigan_gen_checkpoint_3000.pt'\n",
    "\n",
    "checkpoint = torch.load(m_path)\n",
    "train_config = checkpoint['train_setup']\n",
    "sampling_rate = train_config['sampling_rate']\n",
    "gen_config = checkpoint['config']\n",
    "gen_config['num_mel_filters'] = train_config['num_mels']\n",
    "\n",
    "gen = models.get_model('HiFi-GAN', gen_config, DEVICE, forward_is_infer = True)\n",
    "gen.load_state_dict(checkpoint['generator'])\n",
    "gen.remove_weight_norm()\n",
    "gen.eval()\n",
    "\n",
    "denoising_strength = 0.05\n",
    "denoiser = Denoiser(gen, win_length = train_config['win_length'], num_mel_filters = train_config['num_mels']).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259fee3",
   "metadata": {},
   "source": [
    "# Mel spectrum class\n",
    "make it identical to code in training, so we get the same features exactly <br/>\n",
    "NOTE: this is the code used for mel of target audio, for source there is another impl. <br/>\n",
    "TODO: verify and fix if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- NOTE starting 2023-05-28, I added a flag to the dataset-creation script, that makes it use the same mel implementation (unless told explicitly not to)\n",
    "#         for both run-time calculation and save-to-disk calculation\n",
    "\n",
    "#--- this is the implementation used to generate pre-calculated mels for synthetic wavs/fine-tuning (loaded from disk during training)\n",
    "class MelSpec:\n",
    "    def __init__(self, cfg):\n",
    "        filter_length = cfg['filter_length']\n",
    "        hop_length = cfg['hop_length']\n",
    "        win_length = cfg['win_length']\n",
    "        n_mel_channels = cfg['num_mels']\n",
    "        sampling_rate = cfg['sampling_rate']\n",
    "        mel_fmin = cfg['mel_fmin']\n",
    "        mel_fmax = cfg['mel_fmax']\n",
    "        self.stft = layers.TacotronSTFT(filter_length, hop_length, win_length,n_mel_channels, sampling_rate, mel_fmin, mel_fmax)        \n",
    "    \n",
    "    def get_mel(self, audio):\n",
    "        #audio_norm = audio / self.max_wav_value\n",
    "        #audio_norm = audio_norm.unsqueeze(0)\n",
    "        #audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = self.stft.mel_spectrogram(audio)  \n",
    "        \n",
    "        return melspec\n",
    "\n",
    "mel_spec = MelSpec(train_config)\n",
    "\n",
    "#--- this is the implementation used to calculate mel spec of input on-the-fly during training and validation if we are NOT using synthetic wavs/fine-tuning\n",
    "from functools import partial\n",
    "mel_fmax = train_config['mel_fmax'] #--- in train.py, there's option to use different fmax for computing the loss.\n",
    "mel_spec2 = partial(mel_spectrogram, n_fft=train_config['filter_length'],\n",
    "                   num_mels = train_config['num_mels'],\n",
    "                   sampling_rate = train_config['sampling_rate'],\n",
    "                   hop_size = train_config['hop_length'], \n",
    "                   win_size = train_config['win_length'],\n",
    "                   fmin = train_config['mel_fmin'],\n",
    "                   fmax = mel_fmax)\n",
    "\n",
    "if MEL_IMPL == 'fastpitch':\n",
    "    get_mel_spec =  mel_spec.get_mel\n",
    "elif MEL_IMPL == 'hifigan':\n",
    "    get_mel_spec = mel_spec2\n",
    "else:\n",
    "    raise Exception('unknown MEL spec implementation')\n",
    "\n",
    "def array_to_torch(x):\n",
    "    x = torch.FloatTensor(x.astype(np.float32))\n",
    "    x = torch.autograd.Variable(x, requires_grad = False)\n",
    "    x = x.unsqueeze(0)\n",
    "    return x   \n",
    "\n",
    "#--- simple wrapper to apply HiFiGAN generator to input audio\n",
    "def generate_from_audio(x, hifigan_gen, return_numpy_arr = True):\n",
    "    x = array_to_torch(x)    \n",
    "    mel = get_mel_spec(x)\n",
    "        \n",
    "    x_hat = hifigan_gen(mel.cuda())\n",
    "    if return_numpy_arr:\n",
    "        x_hat = x_hat[0].cpu().detach().numpy()[0]\n",
    "    \n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632433d3",
   "metadata": {},
   "source": [
    "# load wav from validation set, get mel and apply model\n",
    "Note: synthesis method should fit the one used to train the model (i.e., \"10 harmonics\" or \"16 khz\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3442e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "flist_validation = open('../data_ssynth/filelists/ssynth_audio_val.txt', 'r').readlines()\n",
    "flist_validation = [fnm.rstrip() for fnm in flist_validation]\n",
    "\n",
    "flist_train = open('../data_ssynth/filelists/ssynth_audio_train.txt', 'r').readlines()\n",
    "flist_train = [fnm.rstrip() for fnm in flist_train]\n",
    "\n",
    "flist = flist_validation #flist_train #\n",
    "n_files = len(flist)\n",
    "\n",
    "#wav_fnm = '../data_ssynth/wavs_synth_10h/01_Free_Improv_dynamic_mic_phrase000.wav'\n",
    "synth_wavs_folder = 'wavs_synth_16k_spl0.5' # 'wavs_synth_10h'\n",
    "\n",
    "mel_loss = np.zeros(n_files)\n",
    "mel_len = np.zeros(n_files)\n",
    "for file_index in tqdm(range(n_files)): #[5] #1\n",
    "    wav_fnm_target = flist[file_index]\n",
    "    y_target, sr, sample_type = load_wav(f'../data_ssynth/{wav_fnm_target}')\n",
    "\n",
    "    wav_fnm = flist[file_index].replace('wavs/', f'{synth_wavs_folder}/')\n",
    "    y, sr, sample_type = load_wav(f'../data_ssynth/{wav_fnm}')\n",
    "\n",
    "    if sample_type == 'PCM_24':\n",
    "        max_wav_value = 2**31 # data type in this case is int32\n",
    "    elif sample_type == 'PCM_16':\n",
    "        max_wav_value = 2**15\n",
    "\n",
    "    #--- convert to float in [-1., 1.]\n",
    "    y = y.astype(np.float32) / np.float32(max_wav_value)\n",
    "    y_target = y_target.astype(np.float32) / np.float32(max_wav_value)\n",
    "\n",
    "    # if DEVICE == 'cuda':\n",
    "    #     y = torch.FloatTensor(y.astype(np.float32))\n",
    "    #     y = torch.autograd.Variable(y, requires_grad = False)\n",
    "    #     y = y.unsqueeze(0)\n",
    "    # else:\n",
    "    #     y = y[np.newaxis, :]\n",
    "    # mel = mel_spec2(y) #mel_spec.get_mel(y)\n",
    "    # y_hat = gen(mel.cuda())\n",
    "    # y_ = y.numpy()[0]\n",
    "\n",
    "    y_hat = generate_from_audio(y, gen, return_numpy_arr = False)\n",
    "    y_hat_den = denoiser(y_hat.squeeze(1), denoising_strength)\n",
    "    y_hat = y_hat[0].cpu().detach().numpy()[0]\n",
    "    y_hat_den = y_hat_den[0].cpu().detach().numpy()[0]\n",
    "\n",
    "    mel_target = get_mel_spec(array_to_torch(y_target)).squeeze(0)\n",
    "    mel_hat = get_mel_spec(array_to_torch(y_hat)).squeeze(0)\n",
    "    mloss = F.l1_loss(mel_target, mel_hat)\n",
    "    mel_loss[file_index] = mloss\n",
    "    mel_len[file_index] = mel_target.shape[1]\n",
    "    #print(f'file {file_index}/{n_files}: mel loss {mloss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_ind = np.argsort(mel_loss)\n",
    "print(sort_ind[0:10],sort_ind[-10:])\n",
    "fig, ax = plt.subplots(figsize = (12,4))\n",
    "ax.plot(mel_len, mel_loss,'.')\n",
    "ax.grid()\n",
    "\n",
    "if False:\n",
    "    fig, ax = plt.subplots(1,2,figsize = (12,4), sharex=True, sharey=True)\n",
    "    ax[0].imshow(mel_target, aspect='auto',interpolation='none',origin='lower')\n",
    "    ax[1].imshow(mel_hat, aspect='auto',interpolation='none',origin='lower')\n",
    "    fig, ax = plt.subplots(figsize = (12,4))\n",
    "    k1,k2 = 360,450 #317 #90\n",
    "    ax.plot(mel_target[:,k1:k2].mean(1))\n",
    "    ax.plot(mel_hat[:,k1:k2].mean(1))\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9eb10",
   "metadata": {},
   "source": [
    "## play result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_normalize = True #False\n",
    "\n",
    "print('Original audio:')\n",
    "ipd.display(ipd.Audio(y_target, rate = sampling_rate, normalize = play_normalize))\n",
    "\n",
    "#print('Synthesized input:')\n",
    "#ipd.display(ipd.Audio(y_, rate = sampling_rate, normalize = play_normalize))\n",
    "\n",
    "print('Generated audio:')\n",
    "ipd.display(ipd.Audio(y_hat, rate = sampling_rate, normalize = play_normalize))\n",
    "\n",
    "print('Generated audio (denoised):')\n",
    "ipd.display(ipd.Audio(y_hat_den, rate = sampling_rate, normalize = play_normalize))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6201cfe9",
   "metadata": {},
   "source": [
    "N1 = 0\n",
    "N2 = 25000\n",
    "fig, ax = plt.subplots(figsize = (12,2))\n",
    "#librosa.display.waveshow(y.numpy(), sr = sampling_rate)\n",
    "\n",
    "ax.plot(y_hat[N1:N2])\n",
    "ax.plot(y_hat_den[N1:N2])\n",
    "ax.plot(y_[N1:N2])\n",
    "#fig, ax = plt.subplots(figsize = (12,2))\n",
    "#librosa.display.waveshow(y_hat, sr = sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c9617",
   "metadata": {},
   "source": [
    "# Try with synthetic input\n",
    "### I define a naive ADSR envelopes with straight lines, probably not the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_harmonics(min_freq_src_hz, max_freq_src_hz, sr, max_freq_tgt_hz):\n",
    "    fmin = max(alto_sax_range[0], min_freq_src_hz) # librosa.note_to_hz(range_notes[0]) # can't naively use fnew.min() since we interpolate to f=0 Hz\n",
    "    \n",
    "    num_harmonics = int(max_freq_tgt_hz / fmin)\n",
    "    new_sr = 2 * max_freq_src_hz * num_harmonics\n",
    "    #--- take the smallest multiple of sr which is high enough (6 is the highest, assuming freqs.max() <= 932 Hz)\n",
    "    new_sr_factor = [k for k in range(1, 10) if k * sr > new_sr][0]\n",
    "    return num_harmonics, new_sr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e99dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import decimate, butter, dlti # resample_poly\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "def additive_synth_sawtooth(freq, env, sampling_rate, additive_synth_k = None, max_freq_hz = None):\n",
    "    ''' TDOO add code to synthesize up to f_max (and not a given number of harmonics)\n",
    "        given input frequency and envelope sampled at sampling_rate, synthesize a band-limited\n",
    "        sawtooth wave using additive synthesis of 10 (or k) harmonies\n",
    "    '''    \n",
    "    #--- set number of harmonics of sawtooth wave\n",
    "    if additive_synth_k is not None:\n",
    "        should_downsample = False\n",
    "        sampling_rate_new = None\n",
    "    else:\n",
    "        num_harmonics, new_sr_factor = get_num_harmonics(freq[freq > 20].min(), freq.max(), sampling_rate, max_freq_hz)\n",
    "        #--- make sure we stay below new nyquist\n",
    "        assert freq.max() * num_harmonics < 0.5 * sampling_rate * new_sr_factor, f'Nyquist says you cannot synthesize {num_harmonics} harmonics at {new_sr_factor} X (current sampling rate)'\n",
    "        additive_synth_k = num_harmonics\n",
    "        sampling_rate_new = sampling_rate * new_sr_factor\n",
    "        should_downsample = True\n",
    "    \n",
    "    dt = 1 / sampling_rate\n",
    "    \n",
    "    #--- interpolate (upsample) to sampling-rate grid, if needed\n",
    "    if sampling_rate_new is not None:\n",
    "        tmax = len(freq) * dt\n",
    "        t_old = np.arange(0, tmax, dt)\n",
    "        fintrp = interp1d(t_old, freq)\n",
    "        dt = 1 / sampling_rate_new\n",
    "        t_new = np.arange(0, tmax, dt)\n",
    "        t_new = t_new[(t_new <= t_old.max()) & (t_new >= t_old.min())] # avoid interpolation out of bounds\n",
    "        freq = fintrp(t_new)  \n",
    "\n",
    "    #--- phase is the integral of instantanous freq\n",
    "    phi = np.cumsum(2 * np.pi * freq * dt)\n",
    "    # to wrap: phi = (phi + np.pi) % (2 * np.pi) - np.pi \n",
    "        \n",
    "    x = np.sin(phi) #(np.sin(phi) + .5*np.sin(2*phi) + .333*np.sin(3*phi) + .25*np.sin(4*phi))\n",
    "    for k in range(2, additive_synth_k + 1):\n",
    "        x += (-1)**(k-1) * np.sin(k * phi) / k\n",
    "    \n",
    "    #--- if we upsampled, go back to original rate\n",
    "    if should_downsample:\n",
    "        #--- for x, give a \"anti-alias\" filter to \"decimate\", but actually use it to filter above the desired max_freq_hz\n",
    "        zpk = butter(12, max_freq_hz, output = 'zpk', fs = sampling_rate_new)\n",
    "        aa_filt = dlti(*zpk) \n",
    "        x = decimate(x, new_sr_factor, ftype = aa_filt)\n",
    "        freq = decimate(freq, new_sr_factor) #--- fnew is just used to zero the envelope, so decimate so size fits\n",
    "        #sr = int(sr / new_sr_factor)\n",
    "    \n",
    "    x *= env\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29934450",
   "metadata": {},
   "source": [
    "## 2 octaves major scale in the range of the alto sax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e675174",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_notes = ['Db3', 'A5'] #['C3', 'A#5'] # alto sax range is ['Db3', 'A5'], take half-step below/above\n",
    "alto_sax_range = librosa.note_to_hz(range_notes)\n",
    "\n",
    "#--- envelope parameters\n",
    "note_len_samples = 24000 #20000 #20000\n",
    "onset_samples = 4500 #3000\n",
    "amp = 0.03\n",
    "amp_sustain = 0.8 # decay envelope to this relative level at the end of the note\n",
    "freq_glide_level = 0.7 #--- during onset, glide into target frequency starting at this pitch (relative)\n",
    "\n",
    "freq = np.zeros(note_len_samples)\n",
    "env = np.zeros(note_len_samples)\n",
    "\n",
    "#--- single note envelope\n",
    "env_single = np.r_[np.linspace(0, 1, onset_samples),  np.linspace(1, amp_sustain, note_len_samples - onset_samples)]\n",
    "env_single = env_single ** 3\n",
    "env_single *= amp\n",
    "\n",
    "\n",
    "#--- major scale in the alto sax range\n",
    "for note in ['D3', 'E3', 'F#3', 'G3', 'A3', 'B3', 'C#4', 'D4', 'E4', 'F#4', 'G4', 'A4', 'B4', 'C#5', 'D5', 'E5', 'F#5', 'G5', 'A5']:\n",
    "    f0 = librosa.note_to_hz(note)\n",
    "    freq_env = np.ones(note_len_samples)\n",
    "    freq_env[:onset_samples] *= np.linspace(freq_glide_level, 1, onset_samples)\n",
    "    \n",
    "    freq = np.r_[freq, f0 * freq_env]\n",
    "    env = np.r_[env, env_single]\n",
    "    \n",
    "freq = np.r_[freq, np.zeros(note_len_samples)]\n",
    "freq[freq <= alto_sax_range[0]] = alto_sax_range[0]\n",
    "freq[freq >= alto_sax_range[1]] = alto_sax_range[1]\n",
    "env = np.r_[env, np.zeros(note_len_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93799417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = additive_synth_sawtooth(freq, env, sampling_rate, additive_synth_k=30)\n",
    "x = additive_synth_sawtooth(freq, env, sampling_rate, max_freq_hz = 16000)\n",
    "#--- in order to apply denoiser, we need the pytorch Tensor, so set return_numpy_arr to False\n",
    "x_hat = generate_from_audio(x, gen, return_numpy_arr = False)\n",
    "\n",
    "x_hat_den = denoiser(x_hat.squeeze(1), 4*denoising_strength)\n",
    "#x = x.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original synthesized input:')\n",
    "ipd.display(ipd.Audio(x, rate = sampling_rate, normalize = play_normalize))\n",
    "\n",
    "print('Generated audio:')\n",
    "x_hat = x_hat[0].cpu().detach().numpy()[0]\n",
    "ipd.display(ipd.Audio(x_hat, rate = sampling_rate, normalize = play_normalize))\n",
    "\n",
    "print('Generated audio (denoised):')\n",
    "x_hat_den = x_hat_den[0].cpu().detach().numpy()[0]\n",
    "ipd.display(ipd.Audio(x_hat, rate = sampling_rate, normalize = play_normalize))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be07671f",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize = (12,2))\n",
    "ax.plot(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d37ff",
   "metadata": {},
   "source": [
    "# Synthesize from parallel audio+midi, and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_seg_inds(arr, shift_end_ind = True):\n",
    "    seg_inds = np.diff(np.r_[0, np.int_(arr), 0]).nonzero()[0]\n",
    "    n_segs = int(seg_inds.shape[0] / 2)\n",
    "    seg_inds = seg_inds.reshape((n_segs, 2)) # + np.c_[np.zeros(n_segs),-np.ones(n_segs)]   \n",
    "    if shift_end_ind:\n",
    "        seg_inds[:,1] -= 1\n",
    "    return seg_inds    \n",
    "\n",
    "def read_midi_to_df(midi_fnm, try_to_fix_note_order = True, time_offset_sec = 0.):\n",
    "    mid = MidiFile(midi_fnm)\n",
    "    \n",
    "    #assert(len(mid.tracks) == 1)\n",
    "    tr = mido.merge_tracks(mid.tracks)\n",
    "    df =  pd.DataFrame([m.dict() for m in tr])\n",
    "    tempo = df.set_index('type').loc['set_tempo','tempo']\n",
    "    if type(tempo) == pd.Series:\n",
    "        uniq_tempo = tempo.unique()\n",
    "        if len(uniq_tempo) > 1:\n",
    "            raise Exception('multiple tempo changes not supported')\n",
    "        else:\n",
    "            tempo = uniq_tempo[0]\n",
    "            \n",
    "    df['ts_sec'] = mido.tick2second(df.time.cumsum(), mid.ticks_per_beat, tempo)\n",
    "    if time_offset_sec != 0.:\n",
    "        df['ts_sec'] += time_offset_sec\n",
    "        df = df[df['ts_sec'] >= 0.]\n",
    "        \n",
    "    #--- extract controls (pitch and aftertouch has seperated messages, not included in CC)\n",
    "    df_aftertouch = df[df.type == 'aftertouch'].dropna(axis = 1).reset_index(drop = True)\n",
    "    df_pitch = df[df.type == 'pitchwheel'].dropna(axis = 1).reset_index(drop = True)\n",
    "    df_cc = df[df.type == 'control_change'].dropna(axis = 1).reset_index(drop = True)\n",
    "    \n",
    "    #--- some mete-messages like \"channel prefix\" contain non-zero time value. so remove them *after* calculating 'ts_sec'\n",
    "    for type_remove in ['channel_prefix', 'track_name', 'instrument_name', 'time_signature', 'key_signature', \n",
    "                        'smpte_offset', 'set_tempo', 'end_of_track', 'midi_port', 'program_change', 'control_change', 'pitchwheel', 'aftertouch', 'marker']:\n",
    "        df = df[df.type != type_remove]\n",
    "    \n",
    "    df = df.dropna(axis = 1).reset_index(drop = True)\n",
    "    \n",
    "    #--- sometimes, instead of a sequence of on-off notes, we get on-on-off-off. try to fix that\n",
    "    if try_to_fix_note_order:\n",
    "        try:\n",
    "            verify_midi(df)\n",
    "        except AssertionError:\n",
    "            print(f'{midi_fnm}: note order problem in midi dataframe, trying to fix...')\n",
    "            df_copy = df.copy()\n",
    "            for ii, inote in df.iterrows():\n",
    "                if inote.type == 'note_on':\n",
    "                    assoc_note_off = df.iloc[ii:].query('type == \"note_off\" and note == @inote.note')\n",
    "                    if len(assoc_note_off) == 0:\n",
    "                        raise Exception('note on with no associated note off')\n",
    "                    inote_off = assoc_note_off.iloc[0]\n",
    "                    inote_off_ind = inote_off.name\n",
    "                    if inote_off_ind > ii + 1:\n",
    "                        next_note_on = df.iloc[ii+1:].query('type == \"note_on\"')\n",
    "                        if len(next_note_on) > 0:\n",
    "                            next_note_on = next_note_on.iloc[0]\n",
    "                            if next_note_on.ts_sec < inote_off.ts_sec:\n",
    "                                df.loc[inote_off_ind, 'ts_sec'] = next_note_on.ts_sec - .001        \n",
    "#             #--- indices of where we expect to see \"note off\" and see \"note on\"\n",
    "#             off_err_ind = df[((df.index % 2) == 1) & (df.type == 'note_on')].index\n",
    "#             for ind in off_err_ind:\n",
    "#                 curr_note = df.loc[ind]\n",
    "#                 next_note = df.loc[ind + 1]\n",
    "#                 prev_note = df.loc[ind - 1]\n",
    "#                 if next_note.type == 'note_off' and next_note.note == prev_note.note:\n",
    "#                     df.loc[ind + 1, 'ts_sec'] = curr_note.ts_sec - 0.001\n",
    "            df = df.sort_values(by = 'ts_sec', kind = 'stable').reset_index(drop = True)\n",
    "            try:\n",
    "                verify_midi(df)\n",
    "                print('fixed')\n",
    "            except AssertionError:\n",
    "                print('fix failed, calling verify_midi() on returned dataframe will fail')\n",
    "                #--- if fix failed, return the original copy\n",
    "                df = df_copy\n",
    "                \n",
    "    return df, df_pitch, df_aftertouch, df_cc\n",
    "\n",
    "def verify_midi(midi_df):\n",
    "    #--- validate the assumption that we have series of note-on/note-off events\n",
    "    assert((midi_df['type'].iloc[::2] == 'note_on').all() and \n",
    "       (midi_df['type'].iloc[1::2] == 'note_off').all() and\n",
    "       (midi_df['note'].iloc[::2].to_numpy() == midi_df['note'].iloc[1::2].to_numpy()).all())\n",
    "\n",
    "def midi_phrase_from_dataframe(p, midi_df, sr):\n",
    "    t0 = p.sample_start / sr\n",
    "    t1 = p.sample_end / sr\n",
    "    midi_p = midi_df[(midi_df.ts_sec >= t0) & (midi_df.ts_sec <= t1)]\n",
    "    \n",
    "    #--- check for missing note_off (at end) or note_on (at start)\n",
    "    first_note = midi_p.iloc[0]\n",
    "    if first_note['type'] == 'note_off':\n",
    "        candidate = midi_df.loc[first_note.name - 1]\n",
    "        if candidate['type'] == 'note_on' and candidate['note'] == first_note['note']:\n",
    "            midi_p = pd.concat([candidate.to_frame().T, midi_p])\n",
    "            \n",
    "    last_note = midi_p.iloc[-1]\n",
    "    if last_note['type'] == 'note_on':\n",
    "        candidate = midi_df.loc[last_note.name + 1]\n",
    "        if candidate['type'] == 'note_off' and candidate['note'] == last_note['note']:\n",
    "            midi_p = pd.concat([midi_p, candidate.to_frame().T])\n",
    "    \n",
    "    return midi_p\n",
    "    \n",
    "def phrase_to_midi_string(p, midi_df, sr):    \n",
    "    midi_p = midi_phrase_from_dataframe(p, midi_df, sr)            \n",
    "    try:\n",
    "        verify_midi(midi_p)\n",
    "    except Exception as e:\n",
    "        print(f'phrase {p.phrase_id} verification failed')\n",
    "        return ''\n",
    "    \n",
    "    note_on = midi_p.loc[midi_p.type == 'note_on']\n",
    "    s = f\"wavs/{p.phrase_id}.wav|{' '.join(note_on.note.astype(int).astype(str).to_list())}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import decimate, butter, dlti # resample_poly\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "range_notes = ['C3', 'A#5'] # alto sax range is ['Db3', 'A5'], take half-step below/above\n",
    "alto_sax_range = librosa.note_to_hz(range_notes)\n",
    "\n",
    "win = 1024\n",
    "ac_win = 512 # autocorrelation window\n",
    "hop = 256\n",
    "\n",
    "def phrase_to_synth(seg, sr, midi_p, t0, num_harmonics = None, max_freq_hz = None, spline_smoothing = None, verbose = False):\n",
    "    ''' Exactly one of these should be given (and the other set to None):\n",
    "            - num harmonics: how many harmonics (inc the fundamental) are used in the saw-tooth additive synthesis\n",
    "                             in this case the max-freq is note-dependent (f0*num_harmonics) and the caller is responsible\n",
    "                             to make sure that (highest note in hz) * (num_harmonics) < nyquist\n",
    "            - max_freq_hz:   synthesize up to this frequency. This is done by upsampling, synthesizing the required amound of harmonics,\n",
    "                             and downsampling back to sr\n",
    "            - smooth_env:    flag to apply smooting using 2nd order splines\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(f'pitch detection range: {alto_sax_range.round(1)} Hz, {(sr/alto_sax_range).astype(int)} samples')\n",
    "        print(f'pitch detection: frame len {win}, auto-corr len {ac_win} (min freq of {sr/ac_win:.1f} Hz), hop len {hop}')\n",
    "    \n",
    "    assert(num_harmonics is None or max_freq_hz is None)\n",
    "    f1, vflag1, vprob1 = librosa.pyin(seg, \n",
    "                                      fmin = alto_sax_range[0], \n",
    "                                      fmax = alto_sax_range[1], \n",
    "                                      sr = sr, \n",
    "                                      frame_length=win, \n",
    "                                      win_length=ac_win, \n",
    "                                      hop_length=hop, \n",
    "                                      center=True, \n",
    "                                      max_transition_rate=100)\n",
    "    times1 = librosa.times_like(f1, sr = sr, hop_length = hop)\n",
    "    no_note1 = (~vflag1)\n",
    "    tmin = times1[0]\n",
    "    tmax = times1[-1]\n",
    "    \n",
    "    note_on = midi_p.loc[midi_p.type == 'note_on']\n",
    "    note_off = midi_p.loc[midi_p.type == 'note_off']\n",
    "    #note_hz = librosa.midi_to_hz(note_on.note)\n",
    "    note_on_ts = note_on['ts_sec'].values - t0\n",
    "    note_off_ts = note_off['ts_sec'].values - t0\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "    #--- interpolate missing pitch, where possible. otherwise, set to 0 (in order to accumulate 0 phase when integrating)\n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "    #--- step A, interpolate within (intra-) midi notes\n",
    "    n_notes = note_on.shape[0]\n",
    "    if verbose:\n",
    "        print(f'samples with non-detected pitch: {np.isnan(f1).sum()}')\n",
    "    for k in range(n_notes):\n",
    "        #--- first, find missing pitch samples which are inside a detected midi note\n",
    "        midi_note_span = (times1 >= note_on_ts[k]) & (times1 <= note_off_ts[k])\n",
    "        \n",
    "        #--- if no missing pitch samples are in the midi note span, we don't need this note, so skip\n",
    "        if not (midi_note_span & no_note1).any():\n",
    "            continue\n",
    "        \n",
    "        #--- if we don't have at least 2 pitch samples in the note span, we can't extrapolate, so skip\n",
    "        if (midi_note_span & ~no_note1).sum() < 2:\n",
    "            continue\n",
    "            \n",
    "        #--- build the interpolating function from detected pitch samples\n",
    "        pitch_intrp = interp1d(times1[midi_note_span & ~no_note1], \n",
    "                               f1[midi_note_span & ~no_note1], \n",
    "                               fill_value = 'extrapolate', \n",
    "                               kind = 'nearest',\n",
    "                               assume_sorted = True)\n",
    "        #--- the time samples where we want to interpolate: inside midi note AND missing pitch\n",
    "        t_intrp = times1[midi_note_span & no_note1]\n",
    "        f1[midi_note_span & no_note1] = pitch_intrp(t_intrp)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'after interpolating using midi notes: samples with non-detected pitch: {np.isnan(f1).sum()}')\n",
    "\n",
    "    #--- step B, interpolate across (inter-) midi notes\n",
    "    max_gap_to_interpolate_sec = 0.1 #--- don't interpolate gaps above this interval in seconds\n",
    "    no_note1 = np.isnan(f1)\n",
    "    seg_inds = binary_array_to_seg_inds(no_note1, shift_end_ind = False)\n",
    "    seg_lens_sec = np.diff(seg_inds, 1)[:,0] * hop / sr\n",
    "    for k, inds in enumerate(seg_inds):\n",
    "        #--- don't interpolate head or tail of signal, or if gap is too long\n",
    "        #--- TODO check energy envelope in gap (interpolate only above env threshold)\n",
    "        gap_len = seg_lens_sec[k]\n",
    "        if (inds[0] == 0) or (inds[1] == len(f1)) or gap_len > max_gap_to_interpolate_sec:\n",
    "            continue\n",
    "        gap_len_samples = inds[1] - inds[0]\n",
    "        if verbose:\n",
    "            print(f'interpolating over {gap_len_samples} samples over gap of {gap_len:.3f} sec')\n",
    "        #--- linear interpolation using 1 sample before and after\n",
    "        new_freqs = np.linspace(f1[inds[0] - 1], f1[inds[1]], gap_len_samples + 2)\n",
    "        f1[inds[0]:inds[1]] = new_freqs[1:-1]\n",
    "\n",
    "    no_note1 = np.isnan(f1)\n",
    "    seg_inds = binary_array_to_seg_inds(no_note1, shift_end_ind = False)\n",
    "    if verbose:\n",
    "        print(f'after interpolating over small gaps: samples with non-detected pitch: {np.isnan(f1).sum()}')\n",
    "    #--- lastly, fill with zeros the samples that are still missing\n",
    "    f1[np.isnan(f1)] = 0.\n",
    "    \n",
    "    #--- set number of harmonics of sawtooth wave\n",
    "    if num_harmonics is not None:\n",
    "        additive_synth_k = num_harmonics # 10\n",
    "        should_downsample = False\n",
    "    else:\n",
    "        num_harmonics, new_sr_factor = get_num_harmonics(f1[f1 > 20].min(), f1.max(), sr, max_freq_hz)\n",
    "        #--- make sure we stay below new nyquist\n",
    "        assert f1.max() * num_harmonics < 0.5 * sr * new_sr_factor, f'Nyquist says you cannot synthesize {num_harmonics} harmonics at {new_sr_factor} X (current sampling rate)'\n",
    "        additive_synth_k = num_harmonics\n",
    "        sr *= new_sr_factor\n",
    "        should_downsample = True\n",
    "    \n",
    "    #--- now interpolate to sampling-rate grid\n",
    "    dt = 1 / sr\n",
    "    fintrp = interp1d(times1, f1)\n",
    "    tnew = np.arange(tmin, tmax, dt)\n",
    "    fnew = fintrp(tnew)\n",
    "    \n",
    "    #--- phase is the integral of instantanous freq\n",
    "    phi = np.cumsum(2 * np.pi * fnew * dt)\n",
    "    # to wrap: phi = (phi + np.pi) % (2 * np.pi) - np.pi \n",
    "        \n",
    "    x = np.sin(phi) #(np.sin(phi) + .5*np.sin(2*phi) + .333*np.sin(3*phi) + .25*np.sin(4*phi))\n",
    "    for k in range(2, additive_synth_k + 1):\n",
    "        x += (-1)**(k-1) * np.sin(k*phi) / k\n",
    "    \n",
    "    #--- if we upsampled, go back to original rate\n",
    "    if should_downsample:\n",
    "        #--- for x, give a \"anti-alias\" filter to \"decimate\", but actually use it to filter above the desired max_freq_hz\n",
    "        zpk = butter(12, max_freq_hz, output = 'zpk', fs = sr)\n",
    "        aa_filt = dlti(*zpk) \n",
    "        x = decimate(x, new_sr_factor, ftype = aa_filt)\n",
    "        fnew = decimate(fnew, new_sr_factor) #--- fnew is just used to zero the envelope, so decimate so size fits\n",
    "        sr = int(sr / new_sr_factor)\n",
    "    \n",
    "    env = librosa.feature.rms(y = seg, frame_length = 512, hop_length = 1, center = True)\n",
    "    env = 1.3 * np.sqrt(2)*env[0, :len(x)]\n",
    "    env[fnew == 0] = 0. # don't apply envelope where there was no pitch found\n",
    "\n",
    "    #--- make envelope go to zero smoothly. This also takes care of the non-continous phase at jumps of f1 to 0\n",
    "    env_segments = binary_array_to_seg_inds(env == 0)\n",
    "    decay_time_sec = 0.05 #--- 50 msec decay time\n",
    "    decay_time_samples = int(decay_time_sec * sr)\n",
    "    for env_seg in env_segments:\n",
    "        if env_seg[0] == 0:\n",
    "            continue\n",
    "        ind_start = max(0, env_seg[0] - decay_time_samples)\n",
    "        decay_len = env_seg[0] - ind_start\n",
    "        decay_factor = np.linspace(1, 0, decay_len)\n",
    "        env[ind_start: env_seg[0]] *= decay_factor  \n",
    "    \n",
    "    if spline_smoothing is not None:\n",
    "        ts = t0 + np.arange(0, len(env)) / sr\n",
    "        spl = UnivariateSpline(ts, env, s = spline_smoothing, k = 2)\n",
    "        env = spl(ts)\n",
    "        env[env < 0.] = 0.\n",
    "        \n",
    "    x *= env\n",
    "    gain = np.sqrt((x**2).mean()) / np.sqrt((seg**2).mean()) \n",
    "    x /= gain\n",
    "    env /= gain\n",
    "    \n",
    "    return x, env, fnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- read phrase info for 1 file\n",
    "file_id = 'Funky_Nadley'\n",
    "midi_fnm = f'../data_ssynth_TMP/midi/{file_id}.mid'\n",
    "print(f'reading midi file {os.path.basename(midi_fnm)}')\n",
    "midi_df, midi_pitch, midi_aftertouch, midi_cc = read_midi_to_df(midi_fnm)\n",
    "verify_midi(midi_df)\n",
    "\n",
    "data_dir = '../data_ssynth_TMP/wavs'\n",
    "phrase_df_fnm = '../data_ssynth_TMP/phrase_df.csv'\n",
    "phrase_df = pd.read_csv(phrase_df_fnm, index_col = 0).reset_index(drop = True)\n",
    "phrase_df = phrase_df[phrase_df.file_nm.str.contains(file_id)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b45a83b",
   "metadata": {},
   "source": [
    "midi_p_cc = midi_phrase_from_dataframe(p, midi_cc, sampling_rate)\n",
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "ax.plot(midi_p_cc.ts_sec, midi_p_cc.value, '.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e590a6",
   "metadata": {},
   "source": [
    "## synthesize using original envelopes of pitch and amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- choose a phrase\n",
    "phrase_ind = 42 #14 #12 #5\n",
    "p = phrase_df.iloc[phrase_ind]\n",
    "\n",
    "t0 = p.sample_start / sampling_rate\n",
    "wav_fnm = f'{data_dir}/{p.phrase_id}.wav'\n",
    "seg, sr = librosa.load(wav_fnm, sr = sampling_rate)\n",
    "midi_p = midi_phrase_from_dataframe(p, midi_df, sampling_rate)\n",
    "midi_p_cc = midi_phrase_from_dataframe(p, midi_cc, sampling_rate)\n",
    "\n",
    "#--- filter 'errors'\n",
    "min_velocity = 3\n",
    "err_notes = (midi_p.type == 'note_on') & (midi_p.velocity <= min_velocity)\n",
    "err_notes.loc[err_notes[err_notes].index + 1] = True #--- add the corresponding note-off\n",
    "midi_p = midi_p[~err_notes]\n",
    "\n",
    "#x, env, freq = phrase_to_synth(seg, sr, midi_p, t0, num_harmonics = 30, spline_smoothing = 2, verbose = False)\n",
    "x, env, freq = phrase_to_synth(seg, sr, midi_p, t0, max_freq_hz=16000, spline_smoothing = .5, verbose = False)\n",
    "\n",
    "#--- apply hifi-gan\n",
    "pre_gain = 0.6\n",
    "x_hat = generate_from_audio(pre_gain * x, gen)\n",
    "ipd.display(ipd.Audio(seg, rate = sr, normalize=play_normalize))\n",
    "ipd.display(ipd.Audio(x_hat, rate = sr, normalize=play_normalize))\n",
    "\n",
    "do_plot = False\n",
    "if do_plot:\n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    ax.plot(seg)\n",
    "    ax.plot(x)\n",
    "    ax.plot(x_hat)\n",
    "    ax.legend(['orig', 'synth_in', 'generated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72ac61",
   "metadata": {},
   "source": [
    "# choose a note and fit (manually...) an ADSR env using cubic-Bezier curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- compare 2 spline smoothing params (0.5 was used for training)\n",
    "x1, env1, freq1 = phrase_to_synth(seg, sr, midi_p, t0, max_freq_hz=16000, spline_smoothing = 2, verbose = False)\n",
    "x2, env2, freq2 = phrase_to_synth(seg, sr, midi_p, t0, max_freq_hz=16000, spline_smoothing = .1, verbose = False)\n",
    "k1, k2 = 52283, 67500\n",
    "env0 = env1[k1:k2] # use this to manually fir an ADSR env using Bezier etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d362cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "\n",
    "def get_bezier_parameters(X, Y, degree=3):\n",
    "    \"\"\" Least square qbezier fit using penrose pseudoinverse.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X: array of x data.\n",
    "    Y: array of y data. Y[0] is the y point for X[0].\n",
    "    degree: degree of the Bézier curve. 2 for quadratic, 3 for cubic.\n",
    "\n",
    "    Based on https://stackoverflow.com/questions/12643079/b%C3%A9zier-curve-fitting-with-scipy\n",
    "    and probably on the 1998 thesis by Tim Andrew Pastva, \"Bézier Curve Fitting\".\n",
    "    \"\"\"\n",
    "    if degree < 1:\n",
    "        raise ValueError('degree must be 1 or greater.')\n",
    "\n",
    "    if len(X) != len(Y):\n",
    "        raise ValueError('X and Y must be of the same length.')\n",
    "\n",
    "    if len(X) < degree + 1:\n",
    "        raise ValueError(f'There must be at least {degree + 1} points to '\n",
    "                         f'determine the parameters of a degree {degree} curve. '\n",
    "                         f'Got only {len(X)} points.')\n",
    "\n",
    "    def bpoly(n, t, k):\n",
    "        \"\"\" Bernstein polynomial when a = 0 and b = 1. \"\"\"\n",
    "        return t ** k * (1 - t) ** (n - k) * comb(n, k)\n",
    "        #return comb(n, i) * ( t**(n-i) ) * (1 - t)**i\n",
    "\n",
    "    def bmatrix(T):\n",
    "        \"\"\" Bernstein matrix for Bézier curves. \"\"\"\n",
    "        return np.matrix([[bpoly(degree, t, k) for k in range(degree + 1)] for t in T])\n",
    "\n",
    "    def least_square_fit(points, M):\n",
    "        M_ = np.linalg.pinv(M)\n",
    "        return M_ * points\n",
    "\n",
    "    T = np.linspace(0, 1, len(X))\n",
    "    M = bmatrix(T)\n",
    "    points = np.array(list(zip(X, Y)))\n",
    "    \n",
    "    final = least_square_fit(points, M).tolist()\n",
    "    final[0] = [X[0], Y[0]]\n",
    "    final[len(final)-1] = [X[len(X)-1], Y[len(Y)-1]]\n",
    "    return final\n",
    "\n",
    "#--- functions copied from: https://stackoverflow.com/questions/12643079/b%C3%A9zier-curve-fitting-with-scipy\n",
    "def bernstein_poly(i, n, t):\n",
    "    \"\"\"\n",
    "     The Bernstein polynomial of n, i as a function of t\n",
    "    \"\"\"\n",
    "    return comb(n, i) * ( t**(n-i) ) * (1 - t)**i\n",
    "\n",
    "\n",
    "def bezier_curve(points, nTimes=50):\n",
    "    \"\"\"\n",
    "       Given a set of control points, return the\n",
    "       bezier curve defined by the control points.\n",
    "\n",
    "       points should be a list of lists, or list of tuples\n",
    "       such as [ [1,1], \n",
    "                 [2,3], \n",
    "                 [4,5], ..[Xn, Yn] ]\n",
    "        nTimes is the number of time steps, defaults to 1000\n",
    "\n",
    "        See http://processingjs.nihongoresources.com/bezierinfo/\n",
    "    \"\"\"\n",
    "\n",
    "    nPoints = len(points)\n",
    "    xPoints = np.array([p[0] for p in points])\n",
    "    yPoints = np.array([p[1] for p in points])\n",
    "\n",
    "    t = np.linspace(0.0, 1.0, nTimes)\n",
    "\n",
    "    polynomial_array = np.array([ bernstein_poly(i, nPoints-1, t) for i in range(0, nPoints)   ])\n",
    "\n",
    "    xvals = np.dot(xPoints, polynomial_array)\n",
    "    yvals = np.dot(yPoints, polynomial_array)\n",
    "\n",
    "    return xvals, yvals\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (14,6))\n",
    "#ax.plot(env1,'.')\n",
    "#ax.plot(env2,':')\n",
    "adsr = np.array([0, 1900, 3600, 8100, len(env0)])\n",
    "n0, n1, n2, n3, n4 = adsr\n",
    "e0, e1, e2, e3, e4 = [env0[k-1] for k in adsr]\n",
    "ax.plot(env0, '.-')\n",
    "\n",
    "a = e1\n",
    "cols = ['b', 'r', 'g', 'c']\n",
    "for k in range(4):\n",
    "    poly = Polygon([[adsr[k], 0], [adsr[k+1], 0], [adsr[k+1], a], [adsr[k],a]], facecolor=cols[k], alpha = 0.15, edgecolor='0.2', closed=True)\n",
    "    ax.add_patch(poly)\n",
    "\n",
    "nA, nD, nS, nR = np.diff(adsr)\n",
    "\n",
    "#curveA = get_bezier_parameters(np.arange(adsr[1]), env0[:nA])\n",
    "#Ax, Ay =  bezier_curve(curveA, nA)\n",
    "\n",
    "#=== Attack\n",
    "Ax, Ay =  bezier_curve([[n0, e0],  [.8 * n1, e0],      [.5 * n1, e1],          [n1, e1]], nA)\n",
    "\n",
    "#=== Decay\n",
    "de1 = (e1 - e2)\n",
    "Dx, Dy =  bezier_curve([[n1, e1], [n1 + .5 * nD, e1], [n2 - .5 * nD, e2 + .4 * de1], [n2, e2]], nD)\n",
    "\n",
    "#--- Sustain\n",
    "de2 = (e2 - e3)\n",
    "Sx, Sy =  bezier_curve([[n2, e2], [n2 + .5 * nD, e2 - .4 * de1], [n3 - .4 * nS, e3 + .1 * de2], [n3, e3]], nS)\n",
    "\n",
    "#--- Release\n",
    "de3 = (e3 - e4)\n",
    "Rx, Ry =  bezier_curve([[n3, e3], [n3 + .4 * nS, e3 - .1 * de2], [n4 - 1.2 * nR, e4 + 0 * de3], [n4, e4]], nR)\n",
    "\n",
    "ax.plot(Ax, Ay, 'r')\n",
    "ax.plot(Dx, Dy, 'r')\n",
    "ax.plot(Sx, Sy, 'r')\n",
    "ax.plot(Rx, Ry, 'r')\n",
    "\n",
    "#--- compare with cubic spline\n",
    "from scipy.interpolate import CubicSpline\n",
    "spl = CubicSpline(adsr, [env0[k-1] for k in adsr], bc_type='clamped')\n",
    "env_spl = spl(np.arange(n4))\n",
    "ax.plot(env_spl, ':')\n",
    "\n",
    "#ax.plot(np.arange(len(env1)) - k1, env1,':')\n",
    "\n",
    "#ax.set_xlim([n1,15000])\n",
    "#ax.set_ylim([.1,.14])\n",
    "#ax.legend(['original envelope', 'piece-wise cubic Bezier', 'cubic spline'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c638c",
   "metadata": {},
   "source": [
    "## synthesize using midi phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa227b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "EWI_TEST = False\n",
    "if EWI_TEST:\n",
    "    midi_fnm = '/home/mlspeech/itamark/ssynth/code/ewimididemo.mid'\n",
    "    #-- we only need pitch and aftertouch, since CC messages from the EWI are identical to aftertouch (CC we get are: 2 - breath control, 7 - main volume, 11 - expression) \n",
    "    #--- midi CC 5 (portamento time) - don't use it at the moment\n",
    "    midi_p, midi_pitch, midi_aftertouch, midi_p_cc = read_midi_to_df(midi_fnm, time_offset_sec = -3.)\n",
    "    min_velocity = 7\n",
    "    err_notes = (midi_p.type == 'note_on') & (midi_p.velocity <= min_velocity)\n",
    "    err_notes.loc[err_notes[err_notes].index + 1] = True #--- add the corresponding note-off\n",
    "    midi_p = midi_p[~err_notes]\n",
    "    #--- clip to alto sax range\n",
    "    note_min, note_max = librosa.hz_to_midi(alto_sax_range)\n",
    "    note_min +=1\n",
    "    note_max -=1\n",
    "    midi_p.loc[midi_p.note < note_min, 'note'] = note_min\n",
    "    midi_p.loc[midi_p.note > note_max, 'note'] = note_max\n",
    "    \n",
    "    #tmin = 18 #3\n",
    "    tmax = np.inf #24.75 #27.75 #18\n",
    "    midi_p, midi_pitch, midi_aftertouch, midi_p_cc = (df[(df.ts_sec <= tmax)] for df in [midi_p, midi_pitch, midi_aftertouch, midi_p_cc])\n",
    "    env_control = midi_aftertouch\n",
    "    freq_control = midi_pitch\n",
    "    freq_control_gain_st = 2 #--- in unit of semi-tones (the pitch-bend at max control value)\n",
    "    seg_len = int((0.5 + midi_p.ts_sec.max()) * sampling_rate)\n",
    "    t0 = 0\n",
    "    use_midi_cc = True\n",
    "    use_midi_pitch = True\n",
    "    gain = 0.07\n",
    "else:\n",
    "    midi_pitch = None\n",
    "    midi_aftertouch = None\n",
    "    env_control = midi_p_cc\n",
    "    seg_len = len(x)\n",
    "    use_midi_cc = True\n",
    "    use_midi_pitch = False\n",
    "    gain = 0.16\n",
    "    \n",
    "#--- parameters\n",
    "\n",
    "attack_time_sec = 15e-3\n",
    "attack_time_samples = int(attack_time_sec * sampling_rate)\n",
    "\n",
    "attack_gain_lin = np.linspace(0, 1, attack_time_samples)\n",
    "attack_gain_quartic = attack_gain_lin ** 4\n",
    "attack_gain_sigmoid = 1 / (1 + np.exp(-10*(attack_gain_lin-.5)))\n",
    "\n",
    "range_to_zero_one = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "attack_gain = range_to_zero_one(attack_gain_sigmoid)\n",
    "\n",
    "num_notes = midi_p.shape[0]\n",
    "\n",
    "freq_midi = np.zeros(seg_len)\n",
    "env_midi = np.zeros(seg_len)\n",
    "\n",
    "for k in np.arange(0, num_notes, 2):\n",
    "    row_on = midi_p.iloc[k]\n",
    "    row_off = midi_p.iloc[k + 1]\n",
    "    \n",
    "    t_on, t_off = row_on.ts_sec, row_off.ts_sec\n",
    "    k_on, k_off = int((t_on - t0) * sampling_rate), int((t_off - t0) * sampling_rate)\n",
    "    cc_note = env_control[(env_control.ts_sec >= t_on) & (env_control.ts_sec <= t_off)]\n",
    "    pitch_note = freq_control[(freq_control.ts_sec >= t_on) & (freq_control.ts_sec <= t_off)]\n",
    "    if len(cc_note) < 3:\n",
    "        continue\n",
    "    note_len = k_off - k_on\n",
    "\n",
    "    #t_on, t_off = row_on.ts_sec - t0, row_off.ts_sec - t0\n",
    "    #k_on, k_off = int(t_on * sampling_rate), int(t_off * sampling_rate)\n",
    "    #note_len = k_off - k_on\n",
    "    \n",
    "    #--- freq env\n",
    "    note_hz = librosa.midi_to_hz(row_on.note)\n",
    "    freq_midi[k_on:k_off] = note_hz #+ 5*np.sin(2*np.pi*15 * np.arange(0,note_len) / sampling_rate)\n",
    "    \n",
    "    #--- amplitude env\n",
    "    note_attack_time_samples = min(int(note_len / 2), attack_time_samples) #--- make sure attack is not longer than the note itself\n",
    "    if not use_midi_cc:\n",
    "        note_gain = gain * row_on.velocity / 128\n",
    "        env_midi[k_on:k_off] = note_gain * np.r_[np.linspace(0, 1, note_attack_time_samples), np.linspace(1, 0, note_len - note_attack_time_samples)]\n",
    "    else:\n",
    "        #env_cc = gain * interp1d(cc_note.ts_sec, cc_note.value, fill_value = 'extrapolate')(np.linspace(t_on, t_off, note_len)) / 128 \n",
    "        env_cc = gain * interp1d(cc_note.ts_sec, cc_note.value, kind='linear', bounds_error=False, fill_value = 0)(np.linspace(t_on, t_off, note_len)) / 128 \n",
    "        env_cc[env_cc < 0] = 0\n",
    "        env_cc[:attack_time_samples] *= attack_gain\n",
    "        env_midi[k_on:k_off] = env_cc\n",
    "        \n",
    "    if use_midi_pitch:\n",
    "        #env_pitch = interp1d(pitch_note.ts_sec, pitch_note.pitch / 2**13, kind='linear', bounds_error=False, fill_value = 'extrapolate')(np.linspace(t_on, t_off, note_len))\n",
    "        if pitch_note.shape[0] > 1:\n",
    "            env_pitch = interp1d(pitch_note.ts_sec, pitch_note.pitch / 2**13, kind='linear', bounds_error=False, fill_value = 'extrapolate')(np.linspace(t_on, t_off, note_len))\n",
    "            freq_mult = 2 ** (freq_control_gain_st * env_pitch / 12)\n",
    "            freq_midi[k_on:k_off] *= freq_mult\n",
    "\n",
    "#x_midi = additive_synth_sawtooth(freq_midi, env_midi, sampling_rate, additive_synth_k=30)\n",
    "x_midi = additive_synth_sawtooth(freq_midi, env_midi, sampling_rate, max_freq_hz = 16000)\n",
    "x_midi_hat = generate_from_audio(x_midi, gen, return_numpy_arr = False)\n",
    "x_midi_hat_den = denoiser(x_midi_hat.squeeze(1), denoising_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929f28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d708ffd9",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "n0 = int(attack_time_samples / 2)\n",
    "attack_gain = np.linspace(0, 1, attack_time_samples)\n",
    "attack_gain_sigmoid = 1 / (1 + np.exp(-10*(attack_gain-.5)))\n",
    "ax.plot(attack_gain)\n",
    "ax.plot(attack_gain**3)\n",
    "ax.plot(attack_gain**4)\n",
    "exp_env = attack_gain_sigmoid #np.exp(attack_gain**2)-1\n",
    "#exp_env /= exp_env[-1]\n",
    "ax.plot(exp_env,':')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "ts = t0 + np.arange(0, len(env_midi)) / sampling_rate\n",
    "ax.plot(ts, env_midi / gain,':.')\n",
    "\n",
    "if midi_aftertouch and midi_pitch:\n",
    "    ax.plot(midi_pitch.ts_sec, midi_pitch.pitch / 4096,'.')\n",
    "    ax.plot(midi_aftertouch.ts_sec, midi_aftertouch.value / 128,'.')\n",
    "else:\n",
    "    \n",
    "for irow, row in midi_p.iterrows():\n",
    "    if row.type == 'note_on':\n",
    "        ax.plot([row.ts_sec, row.ts_sec], [0., 1], 'r-o')\n",
    "    else:\n",
    "        ax.plot([row.ts_sec, row.ts_sec], [0., 0.6], 'c.-')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(['out midi env', 'pitchwheel','aftertouch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EWI_TEST:\n",
    "    print('Original recording:')\n",
    "    ipd.display(ipd.Audio(seg, rate = sr))\n",
    "    print('Sawtooth signal synthesized using real envelopes:')\n",
    "    ipd.display(ipd.Audio(x, rate = sr))\n",
    "    print('Generated audio, using real envelopes:')\n",
    "    #x_hat = x_hat[0].cpu().detach().numpy()[0]\n",
    "    ipd.display(ipd.Audio(x_hat, rate = sampling_rate, normalize = False))\n",
    "\n",
    "print('Sawtooth signal synthesized using \"midi\" envelopes:')\n",
    "ipd.display(ipd.Audio(x_midi, rate = sr))\n",
    "\n",
    "print('Generated audio, using \"midi\" envelopes:')\n",
    "x_midi_hat = x_midi_hat[0].cpu().detach().numpy()[0]\n",
    "ipd.display(ipd.Audio(x_midi_hat, rate = sampling_rate, normalize = False))\n",
    "\n",
    "print('Generated audio, using \"midi\" envelopes (denoised):')\n",
    "x_midi_hat_den = x_midi_hat_den[0].cpu().detach().numpy()[0]\n",
    "ipd.display(ipd.Audio(x_midi_hat_den, rate = sampling_rate, normalize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- plot envelope from origianl recording, and note on/off from parallel midi\n",
    "\n",
    "ts = t0 + np.arange(0, len(x_midi)) / sampling_rate\n",
    "#spl = UnivariateSpline(ts, env, s=.5, k = 2)\n",
    "#env_spline = spl(ts)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "ax.plot(ts[::10],x_midi[::10],'g:')\n",
    "#ax.plot(ts,env,'.')\n",
    "ax.plot(ts,env_midi,'x')\n",
    "#ax.plot(midi_p_cc.ts_sec, midi_p_cc.value / 128 * gain, 'o')\n",
    "ax.plot(midi_aftertouch.ts_sec, midi_aftertouch.value / 128 * gain, 'o')\n",
    "for irow, row in midi_p.iterrows():\n",
    "    if row.type == 'note_on':\n",
    "        ax.plot([row.ts_sec, row.ts_sec], [0., 0.15], 'g:')\n",
    "    else:\n",
    "        ax.plot([row.ts_sec, row.ts_sec], [0., 0.12], 'r.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fad00",
   "metadata": {},
   "source": [
    "# \"APPENDIX\"\n",
    "## Compare MEL spectra of the 2 implementations that are used in the HiFiGAN code\n",
    "(they are not the same :-( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145208b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel1 = mel_spec.get_mel(y)\n",
    "mel2 = mel_spec2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "k = 25\n",
    "ax.plot(mel1[0, :,k], 'bo')\n",
    "ax.plot(mel2[0, :,k], 'r.')\n",
    "ax.legend(['mel-1', 'mel-2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f31e8",
   "metadata": {},
   "source": [
    "## measure timing of mel + inference"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc85c159",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "mel = mel_spec.get_mel(y)\n",
    "y_hat = gen(mel.cuda())[0].cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "980213e0",
   "metadata": {},
   "source": [
    "mel: 7.03 ms ± 258 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "\n",
    "gen: 28.2 ms ± 43.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "mel+gen: 34.8 ms ± 121 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f904ff2b",
   "metadata": {},
   "source": [
    "dur_sec = y.shape[1] / sample_rate\n",
    "t_mel = 7e-3\n",
    "t_gen = 28.2e-3\n",
    "t_total = 34.8e-3\n",
    "print(f'run-time relative to real-time: {t_total / dur_sec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f0d48",
   "metadata": {},
   "source": [
    "## network's impulse response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d07c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = np.r_[np.zeros(sampling_rate) , 1., np.zeros(sampling_rate)].astype(np.float32)\n",
    "ir = generate_from_audio(imp, gen)\n",
    "\n",
    "imp = imp.reshape((1,len(imp)))\n",
    "imp = torch.tensor(imp).clone().detach()\n",
    "imp_mel = mel_spec2(imp).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "#ax.imshow(imp_mel[0], aspect='auto')\n",
    "imp_ = imp.cpu().detach().numpy()[0,40000:50000]\n",
    "#ax.plot(imp_mel[0,:,170:174])\n",
    "ax.plot(ir[40000:50000])\n",
    "ax.plot(0.1*imp_,'r.-')\n",
    "ipd.display(ipd.Audio(ir[40000:50000], rate = sampling_rate, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621aedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "df_cc2 = df_cc[df_cc.control==2]\n",
    "ax.plot(midi_aftertouch.ts_sec, midi_aftertouch.value,'o')\n",
    "ax.plot(df_cc2.ts_sec, df_cc2.value,'x')\n",
    "ax.plot(midi_pitch.ts_sec, midi_pitch.pitch / 2 ** 5,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8989b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97befedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
