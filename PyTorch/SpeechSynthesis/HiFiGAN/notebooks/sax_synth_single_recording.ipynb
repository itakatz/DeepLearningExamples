{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import librosa\n",
    "\n",
    "#--- import HiFiGAN modules\n",
    "sys.path.append('../')\n",
    "import models\n",
    "import common.layers as layers \n",
    "from common.utils import load_wav\n",
    "from hifigan.data_function import mel_spectrogram\n",
    "from hifigan.models import Denoiser\n",
    "\n",
    "import IPython.display as ipd\n",
    "ipd.display(ipd.HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea30139",
   "metadata": {},
   "source": [
    "# load cfg and generator model from checkpoint\n",
    "also create denoiser instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63457324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- get config from checkpoint, so no need to load args from disk\n",
    "#args = pickle.load(open('../TMP_args.p', 'rb'))\n",
    "#gen_config = models.get_model_config('HiFi-GAN', args)\n",
    "\n",
    "DEVICE = 'cuda' # 'cpu' or 'cuda'\n",
    "assert DEVICE == 'cuda', 'ERROR: cpu not supported yet (mel code assumes torch tensors)'\n",
    "\n",
    "m_path = '../results/2023_01_20_hifigan_ssynth44khz_synthesized_input/hifigan_gen_checkpoint_10000.pt'\n",
    "\n",
    "checkpoint = torch.load(m_path)\n",
    "train_config = checkpoint['train_setup']\n",
    "sampling_rate = train_config['sampling_rate']\n",
    "gen_config = checkpoint['config']\n",
    "gen_config['num_mel_filters'] = train_config['num_mels']\n",
    "\n",
    "gen = models.get_model('HiFi-GAN', gen_config, DEVICE, forward_is_infer = True)\n",
    "gen.load_state_dict(checkpoint['generator'])\n",
    "gen.remove_weight_norm()\n",
    "\n",
    "denoising_strength = 0.05\n",
    "denoiser = Denoiser(gen, win_length = train_config['win_length'], num_mel_filters = train_config['num_mels']).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259fee3",
   "metadata": {},
   "source": [
    "# Mel spectrum class\n",
    "make it identical to code in training, so we get the same features exactly <br/>\n",
    "NOTE: this is the code used for mel of target audio, for source there is another impl. <br/>\n",
    "TODO: verify and fix if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpec:\n",
    "    def __init__(self, cfg):\n",
    "        filter_length = cfg['filter_length']\n",
    "        hop_length = cfg['hop_length']\n",
    "        win_length = cfg['win_length']\n",
    "        n_mel_channels = cfg['num_mels']\n",
    "        sampling_rate = cfg['sampling_rate']\n",
    "        mel_fmin = cfg['mel_fmin']\n",
    "        mel_fmax = cfg['mel_fmax']\n",
    "        self.stft = layers.TacotronSTFT(filter_length, hop_length, win_length,n_mel_channels, sampling_rate, mel_fmin, mel_fmax)        \n",
    "    \n",
    "    def get_mel(self, audio):\n",
    "        #audio_norm = audio / self.max_wav_value\n",
    "        #audio_norm = audio_norm.unsqueeze(0)\n",
    "        #audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = self.stft.mel_spectrogram(audio)  \n",
    "        \n",
    "        return melspec\n",
    "\n",
    "mel_spec = MelSpec(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632433d3",
   "metadata": {},
   "source": [
    "## load wav (from validation set) and get mel spectrum\n",
    "Note: synthesis method should fit the one used to train the model (i.e., \"10 harmonics\" or \"16 khz\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist_validation = open('../data_ssynth/filelists/ssynth_audio_val.txt', 'r').readlines()\n",
    "flist_validation = [fnm.rstrip() for fnm in flist_validation]\n",
    "\n",
    "#wav_fnm = '../data_ssynth/wavs_synth_10h/01_Free_Improv_dynamic_mic_phrase000.wav'\n",
    "file_index = 1\n",
    "wav_fnm = flist_validation[file_index].replace('wavs/', 'wavs_synth_10h/')\n",
    "y, sample_rate, sample_type = load_wav(f'../data_ssynth/{wav_fnm}')\n",
    "\n",
    "if sample_type == 'PCM_24':\n",
    "    max_wav_value = 2**31 # data type in this case is int32\n",
    "elif sample_type == 'PCM_16':\n",
    "    max_wav_value = 2**15\n",
    "\n",
    "#--- convert to float in [-1., 1.]\n",
    "y = y.astype(np.float32) / np.float32(max_wav_value)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    y = torch.FloatTensor(y.astype(np.float32))\n",
    "    y = torch.autograd.Variable(y, requires_grad = False)\n",
    "    y = y.unsqueeze(0)\n",
    "else:\n",
    "    y = y[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b975b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = mel_spec.get_mel(y)\n",
    "y_hat = gen(mel.cuda())\n",
    "y_hat_den = denoiser(y_hat.squeeze(1), denoising_strength)\n",
    "\n",
    "y_hat = y_hat[0].cpu().detach().numpy()[0]\n",
    "y_hat_den = y_hat_den[0].cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9eb10",
   "metadata": {},
   "source": [
    "## play result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = y.numpy()[0]\n",
    "\n",
    "print('Original synthesized input:')\n",
    "ipd.display(ipd.Audio(y_, rate = sampling_rate, normalize = False))\n",
    "print('Generated audio:')\n",
    "ipd.display(ipd.Audio(y_hat, rate = sampling_rate, normalize = False))\n",
    "\n",
    "print('Generated audio (denoised):')\n",
    "ipd.display(ipd.Audio(y_hat_den, rate = sampling_rate, normalize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92560f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8be59ac7",
   "metadata": {},
   "source": [
    "N1 = 0\n",
    "N2 = 25000\n",
    "fig, ax = plt.subplots(figsize = (12,2))\n",
    "#librosa.display.waveshow(y.numpy(), sr = sampling_rate)\n",
    "\n",
    "ax.plot(y_hat[N1:N2])\n",
    "ax.plot(y_hat_den[N1:N2])\n",
    "ax.plot(y_[N1:N2])\n",
    "#fig, ax = plt.subplots(figsize = (12,2))\n",
    "#librosa.display.waveshow(y_hat, sr = sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c9617",
   "metadata": {},
   "source": [
    "## Now try with synthetic input\n",
    "### I define a naive ADSR envelopes with straight lines, probably not the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e99dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_synth_sawtooth(freq, env, sampling_rate, additive_synth_k = 10):\n",
    "    ''' given input frequency and envelope sampled at sampling_rate, synthesize a band-limited\n",
    "        sawtooth wave using additive synthesis of 10 (or k) harmonies\n",
    "    '''\n",
    "    dt = 1 / sampling_rate\n",
    "    #--- phase is the integral of instantanous freq\n",
    "    phi = np.cumsum(2 * np.pi * freq * dt)\n",
    "    # to wrap: phi = (phi + np.pi) % (2 * np.pi) - np.pi \n",
    "        \n",
    "    x = np.sin(phi) #(np.sin(phi) + .5*np.sin(2*phi) + .333*np.sin(3*phi) + .25*np.sin(4*phi))\n",
    "    for k in range(2, additive_synth_k + 1):\n",
    "        x += (-1)**(k-1) * np.sin(k * phi) / k\n",
    "    \n",
    "    x *= env\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b6cb5",
   "metadata": {},
   "source": [
    "## 2 octaves major scale in the range of the alto sax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e675174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#range_notes = ['C3', 'A#5'] # alto sax range is ['Db3', 'A5'], take half-step below/above\n",
    "#alto_sax_range = librosa.note_to_hz(range_notes)\n",
    "\n",
    "#--- envelope parameters\n",
    "note_len_samples = 20000 #20000\n",
    "onset_samples = 3000\n",
    "amp = 0.04\n",
    "amp_sustain = 0.5 # decay envelope to this relative level at the end of the note\n",
    "freq_glide_level = 0.8 #--- during onset, glide into target frequency starting at this pitch (relative)\n",
    "\n",
    "freq = np.zeros(note_len_samples)\n",
    "env = np.zeros(note_len_samples)\n",
    "\n",
    "#--- single note envelope\n",
    "env_single = np.r_[np.linspace(0, amp, onset_samples),  np.linspace(amp, amp * amp_sustain, note_len_samples - onset_samples)]\n",
    "\n",
    "#--- major scale in the alto sax range\n",
    "for note in ['D3', 'E3', 'F#3', 'G3', 'A3', 'B3', 'C#4', 'D4', 'E4', 'F#4', 'G4', 'A4', 'B4', 'C#5', 'D5', 'E5', 'F#5', 'G5', 'A5']:\n",
    "    f0 = librosa.note_to_hz(note)\n",
    "    freq_env = np.ones(note_len_samples)\n",
    "    freq_env[:onset_samples] *= np.linspace(freq_glide_level, 1, onset_samples)\n",
    "    freq = np.r_[freq, f0 * freq_env]\n",
    "    env = np.r_[env, env_single]\n",
    "    \n",
    "freq = np.r_[freq, np.zeros(note_len_samples)]\n",
    "env = np.r_[env, np.zeros(note_len_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93799417",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = additive_synth_sawtooth(freq, env, sampling_rate)\n",
    "x = torch.FloatTensor(x.astype(np.float32))\n",
    "x = torch.autograd.Variable(x, requires_grad = False)\n",
    "x = x.unsqueeze(0)\n",
    "mel = mel_spec.get_mel(x)\n",
    "\n",
    "x_hat = gen(mel.cuda())\n",
    "x_hat_den = denoiser(x_hat.squeeze(1), denoising_strength)\n",
    "x = x.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original synthesized input:')\n",
    "ipd.display(ipd.Audio(x, rate = sampling_rate, normalize = False))\n",
    "\n",
    "print('Generated audio:')\n",
    "x_hat = x_hat[0].cpu().detach().numpy()[0]\n",
    "ipd.display(ipd.Audio(x_hat, rate = sampling_rate, normalize = False))\n",
    "\n",
    "print('Generated audio (denoised):')\n",
    "x_hat_den = x_hat_den[0].cpu().detach().numpy()[0]\n",
    "ipd.display(ipd.Audio(x_hat, rate = sampling_rate, normalize = False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8a8fed5",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize = (12,2))\n",
    "x = additive_synth_sawtooth(freq, env, sampling_rate)\n",
    "ax.plot(x)\n",
    "ax.plot(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42893a",
   "metadata": {},
   "source": [
    "## Compare MEL spectra of the 2 implementations that are used in the HiFiGAN code\n",
    "(they are not the same :-( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145208b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- this is the implementation used to calculate mel spec of input during training (and for inference)\n",
    "from functools import partial\n",
    "mel_fmax = train_config['mel_fmax'] #--- in train.py, there's option to use different fmax for computing the loss.\n",
    "mel_spec2 = partial(mel_spectrogram, n_fft=train_config['filter_length'],\n",
    "                   num_mels = train_config['num_mels'],\n",
    "                   sampling_rate = train_config['sampling_rate'],\n",
    "                   hop_size = train_config['hop_length'], \n",
    "                   win_size = train_config['win_length'],\n",
    "                   fmin = train_config['mel_fmin'],\n",
    "                   fmax = mel_fmax)\n",
    "\n",
    "mel1 = mel_spec.get_mel(y)\n",
    "mel2 = mel_spec2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,4))\n",
    "k = 25\n",
    "ax.plot(mel1[0, :,k], 'bo')\n",
    "ax.plot(mel2[0, :,k], 'r.')\n",
    "ax.legend(['mel-1', 'mel-2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aecc2c",
   "metadata": {},
   "source": [
    "## measure timing of mel + inference"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8fdd269",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "mel = mel_spec.get_mel(y)\n",
    "y_hat = gen(mel.cuda())[0].cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5236943",
   "metadata": {},
   "source": [
    "mel: 7.03 ms ± 258 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "\n",
    "gen: 28.2 ms ± 43.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "mel+gen: 34.8 ms ± 121 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d3a3c5a",
   "metadata": {},
   "source": [
    "dur_sec = y.shape[1] / sample_rate\n",
    "t_mel = 7e-3\n",
    "t_gen = 28.2e-3\n",
    "t_total = 34.8e-3\n",
    "print(f'run-time relative to real-time: {t_total / dur_sec}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
